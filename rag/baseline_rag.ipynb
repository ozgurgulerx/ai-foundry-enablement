{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbf67af",
   "metadata": {},
   "source": [
    "### 1.0 Create an Azure AI Search Index \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08b01e",
   "metadata": {},
   "source": [
    " #### Vector-Only Index Creation with Azure AI Search\n",
    "\n",
    "This script creates a vector-only index in Azure AI Search using the General Availability (GA) schema introduced in mid-2024. It sets up an index with just two fields:\n",
    "\n",
    "A string-based document ID (used as the primary key)\n",
    "A vector field (contentVector) that holds embedding data (e.g.Azure OpenAI)\n",
    "We configure the vector search behavior to use the HNSW algorithm with cosine similarity, which is ideal for semantic search scenarios. This vector-only setup is lean and optimized for scenarios where we rely purely on vector search (e.g., similarity search in embeddings) rather than keyword-based retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a5d7199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:29:56 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index02')?api-version=REDACTED'\n",
      "Request method: 'PUT'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '688'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Prefer': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': '2d42c99c-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or updating index 'index02' …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:29:57 [INFO] Response status: 201\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'ETag': '\"0x8DD9F7E122DA5A7\"'\n",
      "    'Location': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '2d42c99c-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 30 May 2025 13:29:57 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Index ready – text + vector fields provisioned\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "create_index_text_and_vector.py\n",
    "───────────────────────────────\n",
    "Creates/updates an Azure AI Search index that stores BOTH:\n",
    "\n",
    "• a searchable **raw** text field (full chunk text)  \n",
    "• a 1 536-d **contentVector** field for HNSW-cosine vector search\n",
    "\n",
    "Everything else is unchanged from the original script—only the extra\n",
    "`raw` field is added so query results can include readable snippets.\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "\n",
    "# ── 1. env ──────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "ENDPOINT   = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "ADMIN_KEY  = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "INDEX_NAME = \"index02\"\n",
    "\n",
    "# ── 2. algorithm + profile (HNSW + cosine) ─────────────────────────\n",
    "algo_cfg = HnswAlgorithmConfiguration(\n",
    "    name=\"hnsw-cosine\",\n",
    "    parameters=HnswParameters(metric=\"cosine\")\n",
    ")\n",
    "\n",
    "profile_cfg = VectorSearchProfile(\n",
    "    name=\"hnsw-cosine-profile\",\n",
    "    algorithm_configuration_name=\"hnsw-cosine\",\n",
    ")\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[algo_cfg],\n",
    "    profiles=[profile_cfg],\n",
    ")\n",
    "\n",
    "# ── 3. schema: id + raw text + vector ──────────────────────────────\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "\n",
    "    # NEW: store the chunk’s plain text so we can preview it in results\n",
    "    SearchField(\n",
    "        name=\"raw\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,         # full-text search enabled\n",
    "        filterable=False,\n",
    "        facetable=False,\n",
    "        sortable=False\n",
    "    ),\n",
    "\n",
    "    SearchField(\n",
    "        name=\"contentVector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1536,\n",
    "        vector_search_profile_name=\"hnsw-cosine-profile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    ")\n",
    "\n",
    "# ── 4. push index ──────────────────────────────────────────────────\n",
    "client = SearchIndexClient(ENDPOINT, AzureKeyCredential(ADMIN_KEY))\n",
    "print(f\"Creating or updating index '{INDEX_NAME}' …\")\n",
    "client.create_or_update_index(index)\n",
    "print(\"✅  Index ready – text + vector fields provisioned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43add243",
   "metadata": {},
   "source": [
    "✅ Result\n",
    "Once this script runs, you’ll have a minimal, production-ready vector-only index that is compatible with the new GA schema and supports efficient vector similarity search via HNSW and cosine distance.\n",
    "\n",
    "You can now upload vectorized documents and perform semantic search queries efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecb4fe",
   "metadata": {},
   "source": [
    "### 2.0 OCR the PDF \n",
    "\n",
    "This code performs OCR on a single PDF file, 2504_IMF_WOO.pdf, using Azure AI Document Intelligence and saves the extracted text as 2504_IMF_WOO.txt in the same directory. It loads API credentials from a .env file, sets up the client, and handles both script and notebook environments by resolving the working directory accordingly. The script submits the PDF to Azure’s prebuilt-read model, waits for the result, extracts text line-by-line from each page, and writes the output as a plain-text file. It includes basic error handling and status messages, making it a clean and reusable OCR workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53afacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍  Processing 2504_IMF_WOO.pdf …\n",
      "✅  Text saved to 2504_IMF_WOO.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "OCR one PDF (2504_IMF_WOO.pdf) with Azure Document Intelligence\n",
    "Saves 2504_IMF_WOO.txt in the same folder.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "\n",
    "# ─────────────── SETUP ───────────────\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent   # works in a .py file\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd()                        # Jupyter fallback\n",
    "\n",
    "load_dotenv(dotenv_path=SCRIPT_DIR / \".env\")       # credentials in .env\n",
    "\n",
    "ENDPOINT = os.getenv(\"DOCUMENTINTELLIGENCE_ENDPOINT\")\n",
    "KEY      = os.getenv(\"DOCUMENTINTELLIGENCE_API_KEY\")\n",
    "if not ENDPOINT or not KEY:\n",
    "    sys.exit(\"❌  Missing DOCUMENTINTELLIGENCE_… values in .env\")\n",
    "\n",
    "client = DocumentIntelligenceClient(\n",
    "    endpoint=ENDPOINT, credential=AzureKeyCredential(KEY)\n",
    ")\n",
    "\n",
    "PDF_FILE = SCRIPT_DIR / \"2504_IMF_WOO.pdf\"\n",
    "if not PDF_FILE.exists():\n",
    "    sys.exit(f\"❗  {PDF_FILE.name} not found in {SCRIPT_DIR.resolve()}\")\n",
    "\n",
    "print(f\"🔍  Processing {PDF_FILE.name} …\")\n",
    "\n",
    "# ─────────────── OCR ───────────────\n",
    "try:\n",
    "    with PDF_FILE.open(\"rb\") as fh:\n",
    "        poller = client.begin_analyze_document(\n",
    "            \"prebuilt-read\", fh, content_type=\"application/pdf\"\n",
    "        )\n",
    "    result = poller.result()\n",
    "\n",
    "    pages_txt = [\n",
    "        \"\\n\".join(ln.content for ln in (p.lines or []))\n",
    "        for p in (result.pages or [])\n",
    "    ]\n",
    "    (PDF_FILE.with_suffix(\".txt\")).write_text(\"\\n\\n\".join(pages_txt), \"utf-8\")\n",
    "    print(f\"✅  Text saved to {PDF_FILE.with_suffix('.txt').name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to process {PDF_FILE.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a85f54",
   "metadata": {},
   "source": [
    "### 3. pre-Processing Text \n",
    "\n",
    "To prepare the OCR dump 2504_IMF_WOO.txt for RAG, the script first joins words split across line-break hyphens (e.g., “eco- \\n nomic” → “economic”). It then strips generic noise—tabs, HTML/Markdown tags, non-UTF8 bytes, divider lines, and bold “IMPORTANT/NOTE” blocks—using regex replacements. Next, it removes IMF-specific clutter such as page headers/footers, Roman- or Arabic-numbered page numbers, table-of-contents lines, chapter titles, and figure/table captions. Finally, it replaces all remaining newlines with spaces and collapses multiple spaces to one, producing a compact, boilerplate-free string that is ideal for tokenization and chunking. The cleaned output is saved as 2504_IMF_WOO.cleaned.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e029b6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Saved cleaned text → 2504_IMF_WOO.cleaned.txt  (624,877 characters)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "clean_2504_imf_woo.py  –  Create a RAG-ready version of 2504_IMF_WOO.txt\n",
    "\n",
    "Reads the raw OCR dump, removes headers/footers, TOC noise, figure captions,\n",
    "hyphen-breaks, HTML/Markdown tags, non-UTF8 chars, etc., and writes\n",
    "2504_IMF_WOO.cleaned.txt in the same directory.\n",
    "\n",
    "Run with:  python clean_2504_imf_woo.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1.  Text-cleaning utility\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Return a compact, boilerplate-free string suitable for chunking.\"\"\"\n",
    "    # fix hyphenated line breaks  (eco-\\n  nomic → economic)\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "\n",
    "    # generic noise\n",
    "    generic = [\n",
    "        r\"\\t\", r\"\\r\\n\", r\"\\r\",                # tabs / CRs\n",
    "        r\"[^\\x00-\\x7F]+\",                     # non-UTF8\n",
    "        r\"<\\/?(table|tr|td|ul|li|p|br)>\",     # HTML tags\n",
    "        r\"\\*\\*IMPORTANT:\\*\\*|\\*\\*NOTE:\\*\\*\", # doc notes\n",
    "        r\"<!|no-loc |text=|<--|-->\",          # markup\n",
    "        r\"```|:::|---|--|###|##|#\",           # md code / hr / headers\n",
    "    ]\n",
    "    for pat in generic:\n",
    "        text = re.sub(pat, \" \", text, flags=re.I)\n",
    "\n",
    "    # IMF-specific headers / footers / TOC lines / captions\n",
    "    imf_noise = [\n",
    "        r\"INTERNATIONAL MONETARY FUND\",\n",
    "        r\"WORLD\\s+ECONOMIC\\s+OUTLOOK\",\n",
    "        r\"\\|\\s*April\\s+\\d{4}\",\n",
    "        r\"^CONTENTS$|^DATA$|^PREFACE$|^FOREWORD$|^EXECUTIVE SUMMARY$\",\n",
    "        r\"^ASSUMPTIONS AND CONVENTIONS$|^FURTHER INFORMATION$|^ERRATA$\",\n",
    "        r\"^Chapter\\s+\\d+.*$\",\n",
    "        r\"^(Table|Figure|Box|Annex)\\s+[A-Z0-9].*$\",\n",
    "        r\"^\\s*[ivxlcdm]+\\s*$\",   # Roman numerals\n",
    "        r\"^\\s*\\d+\\s*$\",          # arabic page nos\n",
    "    ]\n",
    "    for pat in imf_noise:\n",
    "        text = re.sub(pat, \" \", text, flags=re.I | re.M)\n",
    "\n",
    "    # remove remaining newlines → single spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2.  Entrypoint\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def main() -> None:\n",
    "    raw_path = Path.cwd() / \"2504_IMF_WOO.txt\"\n",
    "    if not raw_path.exists():\n",
    "        sys.exit(f\"❌  {raw_path.name} not found in {Path.cwd()}\")\n",
    "\n",
    "    raw_text = raw_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    cleaned   = clean_text(raw_text)\n",
    "\n",
    "    out_path = raw_path.with_suffix(\".cleaned.txt\")\n",
    "    out_path.write_text(cleaned, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"✅  Saved cleaned text → {out_path.name}  \"\n",
    "          f\"({len(cleaned):,} characters)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185a5da",
   "metadata": {},
   "source": [
    "### 4. Chunking Documents for RAG\n",
    "\n",
    "### 🔹 What Is “Chunking” in RAG?\n",
    "\n",
    "*Chunking* means slicing long documents into smaller, self-contained pieces (“chunks”) so they fit the model’s token window and can be embedded, indexed, and retrieved accurately. The aim is to keep **enough context** for a useful answer while **avoiding overly large inputs** that waste tokens or hurt precision.\n",
    "\n",
    "---\n",
    "\n",
    "#### Common Chunking Methods\n",
    "\n",
    "| Method | How it works | Best for |\n",
    "|--------|--------------|----------|\n",
    "| **Fixed-length windows** | Split every *N* tokens/characters, often with 10–20 % overlap. | Logs, code, data dumps where structure ≈ length. |\n",
    "| **Sentence/paragraph split** | Use an NLP splitter; keep full sentences or paragraphs. | Narrative or news text; avoids mid-sentence cuts. |\n",
    "| **Recursive / semantic split** | Split on headings → paragraphs → sentences until each piece < limit (e.g., LangChain `RecursiveCharacterTextSplitter`). | Long structured docs (white papers, legal contracts). |\n",
    "| **Sliding window at retrieval** | No pre-processing; generate overlapping windows on demand around query anchors. | Recall-critical QA (wikis, forums) when storage is cheap. |\n",
    "| **Adaptive / LLM-assisted** | An LLM places boundaries where topics shift. | Highly variable content; experimental but coherent. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Choosing a Strategy\n",
    "\n",
    "* **Code & logs:** fixed 400-token windows + 10 % overlap.  \n",
    "* **Technical reports / legal PDFs:** recursive splitting on headings.  \n",
    "* **Emails & web articles:** paragraph/sentence chunks of ~300-500 tokens.  \n",
    "* **Large wiki corpora:** sliding windows to maximise recall.  \n",
    "* **Mixed formats needing topic coherence:** try LLM-assisted splitting.\n",
    "\n",
    "> **Rule of thumb:** keep chunks **200–800 tokens** and add a small overlap when continuity matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "996150eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:29 [INFO] Splitting document on headings …\n",
      "16:34:29 [INFO] Generated 414 chunks (≈500 tokens each).\n",
      "16:34:29 [INFO] Embedding with text-embedding-3-small …\n",
      "Embedding:   0%|          | 0/26 [00:00<?, ?chunk/s]16:34:30 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:   4%|▍         | 1/26 [00:01<00:25,  1.03s/chunk]16:34:31 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:   8%|▊         | 2/26 [00:01<00:17,  1.37chunk/s]16:34:31 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  12%|█▏        | 3/26 [00:02<00:14,  1.54chunk/s]16:34:32 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  15%|█▌        | 4/26 [00:02<00:12,  1.73chunk/s]16:34:32 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  19%|█▉        | 5/26 [00:03<00:11,  1.81chunk/s]16:34:33 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  23%|██▎       | 6/26 [00:03<00:09,  2.03chunk/s]16:34:33 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  27%|██▋       | 7/26 [00:03<00:08,  2.18chunk/s]16:34:34 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  31%|███       | 8/26 [00:04<00:08,  2.17chunk/s]16:34:34 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  35%|███▍      | 9/26 [00:04<00:08,  2.11chunk/s]16:34:34 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  38%|███▊      | 10/26 [00:05<00:06,  2.29chunk/s]16:34:35 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  42%|████▏     | 11/26 [00:05<00:06,  2.31chunk/s]16:34:35 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  46%|████▌     | 12/26 [00:05<00:05,  2.44chunk/s]16:34:36 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  50%|█████     | 13/26 [00:06<00:05,  2.40chunk/s]16:34:36 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  54%|█████▍    | 14/26 [00:06<00:04,  2.48chunk/s]16:34:36 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  58%|█████▊    | 15/26 [00:07<00:04,  2.54chunk/s]16:34:37 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  62%|██████▏   | 16/26 [00:07<00:04,  2.34chunk/s]16:34:37 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  65%|██████▌   | 17/26 [00:08<00:03,  2.36chunk/s]16:34:38 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  69%|██████▉   | 18/26 [00:08<00:03,  2.34chunk/s]16:34:38 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  73%|███████▎  | 19/26 [00:08<00:02,  2.56chunk/s]16:34:39 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  77%|███████▋  | 20/26 [00:09<00:02,  2.37chunk/s]16:34:39 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  81%|████████  | 21/26 [00:09<00:01,  2.50chunk/s]16:34:39 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  85%|████████▍ | 22/26 [00:09<00:01,  2.59chunk/s]16:34:40 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  88%|████████▊ | 23/26 [00:10<00:01,  2.61chunk/s]16:34:40 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  92%|█████████▏| 24/26 [00:10<00:00,  2.59chunk/s]16:34:40 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  96%|█████████▌| 25/26 [00:11<00:00,  2.36chunk/s]16:34:41 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding: 100%|██████████| 26/26 [00:11<00:00,  2.24chunk/s]\n",
      "16:34:41 [INFO] Uploading to Search index index01 …\n",
      "Uploading:   0%|          | 0/5 [00:00<?, ?chunk/s]16:34:41 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '3643993'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': 'd73a05c8-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "16:34:43 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'd73a05c8-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 30 May 2025 13:34:43 GMT'\n",
      "Uploading:  20%|██        | 1/5 [00:02<00:08,  2.03s/chunk]16:34:43 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '3645620'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': 'd8704fe2-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "16:34:46 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'd8704fe2-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 30 May 2025 13:34:46 GMT'\n",
      "Uploading:  40%|████      | 2/5 [00:04<00:06,  2.32s/chunk]16:34:46 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '3571518'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': 'd9f0fac4-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "16:34:47 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'd9f0fac4-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 30 May 2025 13:34:47 GMT'\n",
      "Uploading:  60%|██████    | 3/5 [00:05<00:03,  1.82s/chunk]16:34:47 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '3512775'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': 'daad7fe6-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "16:34:48 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'daad7fe6-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 30 May 2025 13:34:49 GMT'\n",
      "Uploading:  80%|████████  | 4/5 [00:07<00:01,  1.77s/chunk]16:34:49 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '509866'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': 'dba86ece-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "16:34:49 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'dba86ece-3d5a-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 30 May 2025 13:34:49 GMT'\n",
      "Uploading: 100%|██████████| 5/5 [00:07<00:00,  1.57s/chunk]\n",
      "16:34:49 [INFO] ✅  Done — 414 chunks embedded & indexed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "embed_and_upload_chunks.py\n",
    "──────────────────────────\n",
    "• Reads IMF WEO cleaned text (2504_IMF_WOO.cleaned.txt)\n",
    "• Splits it into ≈ 500-token chunks (10 % overlap, heading-aware)\n",
    "• Embeds each chunk with Azure OpenAI\n",
    "• **Uploads id + raw + contentVector** to Azure AI Search index01\n",
    "• Shows INFO logging and tqdm progress bars\n",
    "\n",
    "(raw text is now retained so query results can include readable snippets)\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, re, logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tiktoken\n",
    "import openai\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# ── 1. logging ────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"IMF-Embed\")\n",
    "\n",
    "# ── 2. env vars ───────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "\n",
    "SEARCH_ENDPOINT   = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "SEARCH_ADMIN_KEY  = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"index01\")\n",
    "if not SEARCH_ENDPOINT or not SEARCH_ADMIN_KEY:\n",
    "    sys.exit(\"❌  Missing AZURE_SEARCH_* vars in .env\")\n",
    "\n",
    "AOAI_ENDPOINT     = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\").rstrip(\"/\")\n",
    "AOAI_KEY          = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AOAI_API_VERSION  = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "EMBED_DEPLOYMENT  = os.getenv(\"AZURE_TEXT_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "if not AOAI_ENDPOINT or not AOAI_KEY or not EMBED_DEPLOYMENT:\n",
    "    sys.exit(\"❌  Missing Azure OpenAI vars in .env\")\n",
    "\n",
    "# ── 3. files & params ────────────────────────────────────────────\n",
    "CLEAN_FILE   = Path(\"2504_IMF_WOO.cleaned.txt\")\n",
    "CHUNK_TOKENS = 500\n",
    "OVERLAP      = 50\n",
    "EMB_BATCH    = 16\n",
    "UPL_BATCH    = 100\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# ── 4. helpers ────────────────────────────────────────────────────\n",
    "def slide(tokens, size, step):\n",
    "    for i in range(0, len(tokens), step):\n",
    "        yield tokens[i : i + size]\n",
    "\n",
    "def chunkify(text: str, parent: str = \"WEO25\"):\n",
    "    step = CHUNK_TOKENS - OVERLAP\n",
    "    tokens = enc.encode(text)\n",
    "    for idx, win in enumerate(slide(tokens, CHUNK_TOKENS, step)):\n",
    "        yield {\n",
    "            \"id\": f\"{parent}_c{idx:06}\",\n",
    "            \"raw\": enc.decode(win),          # kept for query-time snippets\n",
    "            \"@search.action\": \"upload\",\n",
    "        }\n",
    "\n",
    "# ── 5. load cleaned text ─────────────────────────────────────────\n",
    "if not CLEAN_FILE.exists():\n",
    "    sys.exit(\"❌  Cleaned text file not found\")\n",
    "\n",
    "full_text = CLEAN_FILE.read_text(\"utf-8\")\n",
    "\n",
    "log.info(\"Splitting document on headings …\")\n",
    "blocks = re.split(r\"\\n([A-Z][^\\n]{3,100})\\n\", full_text)  # even=text, odd=heading\n",
    "\n",
    "chunks = []\n",
    "for i in range(0, len(blocks), 2):\n",
    "    body = blocks[i]\n",
    "    chunks.extend(chunkify(body))\n",
    "\n",
    "log.info(\"Generated %s chunks (≈%s tokens each).\", len(chunks), CHUNK_TOKENS)\n",
    "\n",
    "# ── 6. embed ─────────────────────────────────────────────────────\n",
    "openai_client = openai.AzureOpenAI(\n",
    "    api_key     = AOAI_KEY,\n",
    "    api_version = AOAI_API_VERSION,\n",
    "    base_url    = f\"{AOAI_ENDPOINT}/openai/deployments/{EMBED_DEPLOYMENT}\",\n",
    ")\n",
    "\n",
    "log.info(\"Embedding with %s …\", EMBED_DEPLOYMENT)\n",
    "for i in tqdm(range(0, len(chunks), EMB_BATCH), desc=\"Embedding\", unit=\"chunk\"):\n",
    "    batch  = chunks[i:i+EMB_BATCH]\n",
    "    inputs = [c[\"raw\"] for c in batch]\n",
    "    resp   = openai_client.embeddings.create(model=EMBED_DEPLOYMENT, input=inputs)\n",
    "    for rec, emb in zip(batch, resp.data):\n",
    "        rec[\"contentVector\"] = emb.embedding  # name matches index field\n",
    "\n",
    "# ── 7. upload ────────────────────────────────────────────────────\n",
    "search = SearchClient(\n",
    "    endpoint    = SEARCH_ENDPOINT,\n",
    "    index_name  = SEARCH_INDEX_NAME,\n",
    "    api_version = \"2024-07-01\",\n",
    "    credential  = AzureKeyCredential(SEARCH_ADMIN_KEY),\n",
    ")\n",
    "\n",
    "log.info(\"Uploading to Search index %s …\", SEARCH_INDEX_NAME)\n",
    "for i in tqdm(range(0, len(chunks), UPL_BATCH), desc=\"Uploading\", unit=\"chunk\"):\n",
    "    batch   = chunks[i:i+UPL_BATCH]\n",
    "    results = search.upload_documents(batch)\n",
    "    fails   = [r for r in results if not r.succeeded]\n",
    "    if fails:\n",
    "        log.warning(\"%d failures (starting at %s)\", len(fails), batch[0][\"id\"])\n",
    "\n",
    "log.info(\"✅  Done — %s chunks embedded & indexed.\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0456319",
   "metadata": {},
   "source": [
    "This script automates the pipeline for turning the **cleaned IMF WEO report** into a searchable vector index. It loads *2504_IMF_WOO.cleaned.txt*, splits the text into ~500-token chunks with a 10 % overlap (to avoid cutting important context), and sends each chunk to Azure OpenAI’s **text-embedding-3-small** model to obtain a 1 536-dimensional vector. After embedding, the script uploads only two fields—`id` and `contentVector`—to the Azure AI Search index **index01**, which is set up as a vector-only schema. All credentials come from the `.env` file, and `tqdm` progress bars plus INFO-level logging make it easy to track embedding and upload progress inside a Jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de288b90",
   "metadata": {},
   "source": [
    "### 5. Query the Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a6d7149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:36:07 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "16:36:07 [WARNING] k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizedQuery'> and will be ignored\n",
      "16:36:07 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.post.search?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '34274'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': '0a883274-3d5b-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "16:36:08 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '0a883274-3d5b-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Fri, 30 May 2025 13:36:08 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WEO25_c000087  (score 0.765)\n",
      "(2030e) Sources: International Energy Agency (IEA); Organization of the Petroleum Exporting Countries (OPEC); and IMF staff calculations. Note: Estimates for data centers (DCs) and electric vehicles (EVs) are for the world and come from OPEC and the IEA, respectively. Data labels in the figure use\n",
      "\n",
      "WEO25_c000088  (score 0.759)\n",
      "greater use of compute by companies pursuing better-performing models (Hoffmann and others 2022). Adding to this complexity is the recent emergence of reasoning models-which require more compute in their deployment-and possibly greater AI use driven by lower costs and availability of open-source mo\n",
      "\n",
      "WEO25_c000089  (score 0.741)\n",
      "4). 8 percent in the United States (525 TWh), 3 percent in Europe (145 TWh), and 2 percent in China (237 TWh) relative to the baseline scenario. In the AI scenario under alternative energy policies, the increase in total electricity supply is kept the same, but its composition shifts in favor of ren\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:36:11 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── LLM Answer ──\n",
      "\n",
      "AI is expected to significantly increase future energy demand, primarily through higher electricity consumption by data centers and AI services. By 2030, global electricity consumption from AI could reach 1,500 TWh—comparable to India's current total electricity use and about 1.5 times higher than projected demand from electric vehicles. In the United States, electricity demand from data centers is projected to more than triple from 178 TWh in 2024 to 606 TWh in 2030. Under an AI-driven scenario, total electricity supply is expected to rise by 8% in the US, 3% in Europe, and 2% in China relative to baseline projections. This surge in demand may drive up electricity prices and could require significant investments in renewables and grid infrastructure to avoid supply bottlenecks and price spikes. If renewables scale-up or grid investments lag, price increases could be substantial, and electricity might need to be redirected from other sectors, impacting energy-intensive industries negatively [WEO25_c000087], [WEO25_c000088], [WEO25_c000089].\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "rag_query.py\n",
    "────────────\n",
    "1. Embed a natural-language question\n",
    "2. Retrieve the TOP_K most similar chunks from Azure AI Search\n",
    "3. Feed those chunks to a chat-capable Azure OpenAI model\n",
    "4. Print the LLM’s grounded answer + show which chunks were used\n",
    "\"\"\"\n",
    "\n",
    "import os, textwrap, openai\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# ── 0. config ──────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "\n",
    "QUESTION     = \"How will AI affect future energy demand according to the report?\"\n",
    "TOP_K        = 3\n",
    "VECTOR_FIELD = \"contentVector\"\n",
    "\n",
    "EMBED_DEPLOY = os.getenv(\"AZURE_TEXT_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "CHAT_DEPLOY  = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "                         os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"))  # fallback\n",
    "AOAI_BASE    = os.getenv(\"AZURE_OPENAI_ENDPOINT\").rstrip(\"/\")\n",
    "AOAI_KEY     = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AOAI_VER     = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "# ── 1. embed the question ─────────────────────────────────────────\n",
    "aoai = openai.AzureOpenAI(\n",
    "    api_key=AOAI_KEY,\n",
    "    api_version=AOAI_VER,\n",
    "    base_url=f\"{AOAI_BASE}/openai/deployments/{EMBED_DEPLOY}\"\n",
    ")\n",
    "embedding = aoai.embeddings.create(model=EMBED_DEPLOY, input=[QUESTION]).data[0].embedding\n",
    "\n",
    "# ── 2. vector search ──────────────────────────────────────────────\n",
    "search = SearchClient(\n",
    "    endpoint   = os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"index01\"),\n",
    "    credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")),\n",
    "    api_version=\"2024-07-01\"\n",
    ")\n",
    "\n",
    "vector_query = VectorizedQuery(vector=embedding, fields=VECTOR_FIELD, k=TOP_K)\n",
    "hits = list(search.search(search_text=\"\", vector_queries=[vector_query], top=TOP_K))\n",
    "\n",
    "contexts = []\n",
    "for h in hits:\n",
    "    snippet = (h.get(\"raw\") or \"\")[:300].replace(\"\\n\", \" \")\n",
    "    contexts.append(h.get(\"raw\", \"\"))\n",
    "    print(f\"\\n{h['id']}  (score {h['@search.score']:.3f})\")\n",
    "    print(textwrap.shorten(snippet, 300) if snippet else \"(raw text not stored)\")\n",
    "\n",
    "if not contexts:\n",
    "    print(\"\\nNo chunks retrieved – cannot answer.\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ── 3. ask the LLM with retrieved context ─────────────────────────\n",
    "chat = openai.AzureOpenAI(\n",
    "    api_key=AOAI_KEY,\n",
    "    api_version=AOAI_VER,\n",
    "    base_url=f\"{AOAI_BASE}/openai/deployments/{CHAT_DEPLOY}\"\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"You are an analyst answering questions using the provided report excerpts.\n",
    "Answer concisely and cite excerpts by chunk id when relevant.\n",
    "\n",
    "Question:\n",
    "{QUESTION}\n",
    "\n",
    "Excerpts:\n",
    "{\"\\n\\n\".join(f\"[{hits[i]['id']}]\\n{contexts[i]}\" for i in range(len(contexts)))}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = chat.chat.completions.create(\n",
    "    model=CHAT_DEPLOY,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=400,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"\\n── LLM Answer ──\\n\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
