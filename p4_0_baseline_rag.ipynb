{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbf67af",
   "metadata": {},
   "source": [
    "### 1.0 Create an Azure AI Search Index \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08b01e",
   "metadata": {},
   "source": [
    " #### Vector-Only Index Creation with Azure AI Search\n",
    "\n",
    "This script creates a vector-only index in Azure AI Search using the General Availability (GA) schema introduced in mid-2024. It sets up an index with just two fields:\n",
    "\n",
    "A string-based document ID (used as the primary key)\n",
    "A vector field (contentVector) that holds embedding data (e.g.Azure OpenAI)\n",
    "We configure the vector search behavior to use the HNSW algorithm with cosine similarity, which is ideal for semantic search scenarios. This vector-only setup is lean and optimized for scenarios where we rely purely on vector search (e.g., similarity search in embeddings) rather than keyword-based retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a5d7199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or updating index 'index01' …\n",
      "✅  Vector-only index ready\n"
     ]
    }
   ],
   "source": [
    "# create_index_vector_only.py – GA-compatible vector-only index\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "\n",
    "# ── 1. env ──────────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "ENDPOINT   = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "ADMIN_KEY  = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"index01\")\n",
    "\n",
    "# ── 2. algorithm + profile (HNSW + cosine) ─────────────────────────────\n",
    "algo_cfg = HnswAlgorithmConfiguration(\n",
    "    name=\"hnsw-cosine\",\n",
    "    parameters=HnswParameters(metric=\"cosine\")  # defaults (m=4, ef* etc.)\n",
    ")\n",
    "\n",
    "profile_cfg = VectorSearchProfile(           # ← referenced by the field\n",
    "    name=\"hnsw-cosine-profile\",\n",
    "    algorithm_configuration_name=\"hnsw-cosine\",\n",
    ")\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[algo_cfg],\n",
    "    profiles=[profile_cfg],\n",
    ")\n",
    "\n",
    "# ── 3. schema: key + vector field only ─────────────────────────────────\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchField(\n",
    "        name=\"contentVector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1536,\n",
    "        vector_search_profile_name=\"hnsw-cosine-profile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    ")\n",
    "\n",
    "# ── 4. push the index ──────────────────────────────────────────────────\n",
    "client = SearchIndexClient(endpoint=ENDPOINT, credential=AzureKeyCredential(ADMIN_KEY))\n",
    "print(f\"Creating or updating index '{INDEX_NAME}' …\")\n",
    "client.create_or_update_index(index)\n",
    "print(\"✅  Vector-only index ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43add243",
   "metadata": {},
   "source": [
    "✅ Result\n",
    "Once this script runs, you’ll have a minimal, production-ready vector-only index that is compatible with the new GA schema and supports efficient vector similarity search via HNSW and cosine distance.\n",
    "\n",
    "You can now upload vectorized documents and perform semantic search queries efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecb4fe",
   "metadata": {},
   "source": [
    "### 2.0 OCR the PDF \n",
    "\n",
    "This code performs OCR on a single PDF file, 2504_IMF_WOO.pdf, using Azure AI Document Intelligence and saves the extracted text as 2504_IMF_WOO.txt in the same directory. It loads API credentials from a .env file, sets up the client, and handles both script and notebook environments by resolving the working directory accordingly. The script submits the PDF to Azure’s prebuilt-read model, waits for the result, extracts text line-by-line from each page, and writes the output as a plain-text file. It includes basic error handling and status messages, making it a clean and reusable OCR workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53afacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍  Processing 2504_IMF_WOO.pdf …\n",
      "✅  Text saved to 2504_IMF_WOO.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "OCR one PDF (2504_IMF_WOO.pdf) with Azure Document Intelligence\n",
    "Saves 2504_IMF_WOO.txt in the same folder.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "\n",
    "# ─────────────── SETUP ───────────────\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent   # works in a .py file\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd()                        # Jupyter fallback\n",
    "\n",
    "load_dotenv(dotenv_path=SCRIPT_DIR / \".env\")       # credentials in .env\n",
    "\n",
    "ENDPOINT = os.getenv(\"DOCUMENTINTELLIGENCE_ENDPOINT\")\n",
    "KEY      = os.getenv(\"DOCUMENTINTELLIGENCE_API_KEY\")\n",
    "if not ENDPOINT or not KEY:\n",
    "    sys.exit(\"❌  Missing DOCUMENTINTELLIGENCE_… values in .env\")\n",
    "\n",
    "client = DocumentIntelligenceClient(\n",
    "    endpoint=ENDPOINT, credential=AzureKeyCredential(KEY)\n",
    ")\n",
    "\n",
    "PDF_FILE = SCRIPT_DIR / \"2504_IMF_WOO.pdf\"\n",
    "if not PDF_FILE.exists():\n",
    "    sys.exit(f\"❗  {PDF_FILE.name} not found in {SCRIPT_DIR.resolve()}\")\n",
    "\n",
    "print(f\"🔍  Processing {PDF_FILE.name} …\")\n",
    "\n",
    "# ─────────────── OCR ───────────────\n",
    "try:\n",
    "    with PDF_FILE.open(\"rb\") as fh:\n",
    "        poller = client.begin_analyze_document(\n",
    "            \"prebuilt-read\", fh, content_type=\"application/pdf\"\n",
    "        )\n",
    "    result = poller.result()\n",
    "\n",
    "    pages_txt = [\n",
    "        \"\\n\".join(ln.content for ln in (p.lines or []))\n",
    "        for p in (result.pages or [])\n",
    "    ]\n",
    "    (PDF_FILE.with_suffix(\".txt\")).write_text(\"\\n\\n\".join(pages_txt), \"utf-8\")\n",
    "    print(f\"✅  Text saved to {PDF_FILE.with_suffix('.txt').name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to process {PDF_FILE.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a85f54",
   "metadata": {},
   "source": [
    "### 3. pre-Processing Text \n",
    "\n",
    "To prepare the OCR dump 2504_IMF_WOO.txt for RAG, the script first joins words split across line-break hyphens (e.g., “eco- \\n nomic” → “economic”). It then strips generic noise—tabs, HTML/Markdown tags, non-UTF8 bytes, divider lines, and bold “IMPORTANT/NOTE” blocks—using regex replacements. Next, it removes IMF-specific clutter such as page headers/footers, Roman- or Arabic-numbered page numbers, table-of-contents lines, chapter titles, and figure/table captions. Finally, it replaces all remaining newlines with spaces and collapses multiple spaces to one, producing a compact, boilerplate-free string that is ideal for tokenization and chunking. The cleaned output is saved as 2504_IMF_WOO.cleaned.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e029b6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Saved cleaned text → 2504_IMF_WOO.cleaned.txt  (624,877 characters)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "clean_2504_imf_woo.py  –  Create a RAG-ready version of 2504_IMF_WOO.txt\n",
    "\n",
    "Reads the raw OCR dump, removes headers/footers, TOC noise, figure captions,\n",
    "hyphen-breaks, HTML/Markdown tags, non-UTF8 chars, etc., and writes\n",
    "2504_IMF_WOO.cleaned.txt in the same directory.\n",
    "\n",
    "Run with:  python clean_2504_imf_woo.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1.  Text-cleaning utility\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Return a compact, boilerplate-free string suitable for chunking.\"\"\"\n",
    "    # fix hyphenated line breaks  (eco-\\n  nomic → economic)\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "\n",
    "    # generic noise\n",
    "    generic = [\n",
    "        r\"\\t\", r\"\\r\\n\", r\"\\r\",                # tabs / CRs\n",
    "        r\"[^\\x00-\\x7F]+\",                     # non-UTF8\n",
    "        r\"<\\/?(table|tr|td|ul|li|p|br)>\",     # HTML tags\n",
    "        r\"\\*\\*IMPORTANT:\\*\\*|\\*\\*NOTE:\\*\\*\", # doc notes\n",
    "        r\"<!|no-loc |text=|<--|-->\",          # markup\n",
    "        r\"```|:::|---|--|###|##|#\",           # md code / hr / headers\n",
    "    ]\n",
    "    for pat in generic:\n",
    "        text = re.sub(pat, \" \", text, flags=re.I)\n",
    "\n",
    "    # IMF-specific headers / footers / TOC lines / captions\n",
    "    imf_noise = [\n",
    "        r\"INTERNATIONAL MONETARY FUND\",\n",
    "        r\"WORLD\\s+ECONOMIC\\s+OUTLOOK\",\n",
    "        r\"\\|\\s*April\\s+\\d{4}\",\n",
    "        r\"^CONTENTS$|^DATA$|^PREFACE$|^FOREWORD$|^EXECUTIVE SUMMARY$\",\n",
    "        r\"^ASSUMPTIONS AND CONVENTIONS$|^FURTHER INFORMATION$|^ERRATA$\",\n",
    "        r\"^Chapter\\s+\\d+.*$\",\n",
    "        r\"^(Table|Figure|Box|Annex)\\s+[A-Z0-9].*$\",\n",
    "        r\"^\\s*[ivxlcdm]+\\s*$\",   # Roman numerals\n",
    "        r\"^\\s*\\d+\\s*$\",          # arabic page nos\n",
    "    ]\n",
    "    for pat in imf_noise:\n",
    "        text = re.sub(pat, \" \", text, flags=re.I | re.M)\n",
    "\n",
    "    # remove remaining newlines → single spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2.  Entrypoint\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def main() -> None:\n",
    "    raw_path = Path.cwd() / \"2504_IMF_WOO.txt\"\n",
    "    if not raw_path.exists():\n",
    "        sys.exit(f\"❌  {raw_path.name} not found in {Path.cwd()}\")\n",
    "\n",
    "    raw_text = raw_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    cleaned   = clean_text(raw_text)\n",
    "\n",
    "    out_path = raw_path.with_suffix(\".cleaned.txt\")\n",
    "    out_path.write_text(cleaned, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"✅  Saved cleaned text → {out_path.name}  \"\n",
    "          f\"({len(cleaned):,} characters)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185a5da",
   "metadata": {},
   "source": [
    "### 4. Chunking Documents for RAG\n",
    "\n",
    "### 🔹 What Is “Chunking” in RAG?\n",
    "\n",
    "*Chunking* means slicing long documents into smaller, self-contained pieces (“chunks”) so they fit the model’s token window and can be embedded, indexed, and retrieved accurately. The aim is to keep **enough context** for a useful answer while **avoiding overly large inputs** that waste tokens or hurt precision.\n",
    "\n",
    "---\n",
    "\n",
    "#### Common Chunking Methods\n",
    "\n",
    "| Method | How it works | Best for |\n",
    "|--------|--------------|----------|\n",
    "| **Fixed-length windows** | Split every *N* tokens/characters, often with 10–20 % overlap. | Logs, code, data dumps where structure ≈ length. |\n",
    "| **Sentence/paragraph split** | Use an NLP splitter; keep full sentences or paragraphs. | Narrative or news text; avoids mid-sentence cuts. |\n",
    "| **Recursive / semantic split** | Split on headings → paragraphs → sentences until each piece < limit (e.g., LangChain `RecursiveCharacterTextSplitter`). | Long structured docs (white papers, legal contracts). |\n",
    "| **Sliding window at retrieval** | No pre-processing; generate overlapping windows on demand around query anchors. | Recall-critical QA (wikis, forums) when storage is cheap. |\n",
    "| **Adaptive / LLM-assisted** | An LLM places boundaries where topics shift. | Highly variable content; experimental but coherent. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Choosing a Strategy\n",
    "\n",
    "* **Code & logs:** fixed 400-token windows + 10 % overlap.  \n",
    "* **Technical reports / legal PDFs:** recursive splitting on headings.  \n",
    "* **Emails & web articles:** paragraph/sentence chunks of ~300-500 tokens.  \n",
    "* **Large wiki corpora:** sliding windows to maximise recall.  \n",
    "* **Mixed formats needing topic coherence:** try LLM-assisted splitting.\n",
    "\n",
    "> **Rule of thumb:** keep chunks **200–800 tokens** and add a small overlap when continuity matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996150eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "embed_and_upload_chunks.py\n",
    "──────────────────────────\n",
    "• Reads IMF WEO cleaned text (weo25_clean.txt) + page_map (weo25_page_map.json)\n",
    "• Splits into ≈500-token chunks (10 % overlap, heading-aware)\n",
    "• Embeds with Azure OpenAI (text-embedding-3-small, or whatever you set)\n",
    "• Uploads to Azure AI Search index01\n",
    "• Shows INFO logging + tqdm progress bars (Jupyter-friendly)\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, json, re, logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tiktoken               # pip install tiktoken\n",
    "import openai                 # pip install openai>=1.3\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# ── 1.  Logging ────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"IMF-Embed\")\n",
    "\n",
    "# ── 2.  Load .env & pull parameters ───────────────────────────────\n",
    "load_dotenv()  # reads .env in cwd\n",
    "\n",
    "# Azure Search\n",
    "SEARCH_ENDPOINT   = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "SEARCH_ADMIN_KEY  = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"index01\")\n",
    "if not SEARCH_ENDPOINT or not SEARCH_ADMIN_KEY:\n",
    "    sys.exit(\"❌  Set AZURE_SEARCH_ENDPOINT and AZURE_SEARCH_ADMIN_KEY in .env\")\n",
    "\n",
    "# Azure OpenAI\n",
    "AOAI_ENDPOINT     = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\").rstrip(\"/\")\n",
    "AOAI_KEY          = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AOAI_API_VERSION  = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "EMBED_DEPLOYMENT  = os.getenv(\"AZURE_TEXT_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "if not AOAI_ENDPOINT or not AOAI_KEY or not EMBED_DEPLOYMENT:\n",
    "    sys.exit(\"❌  Missing Azure OpenAI env vars (endpoint/key/deployment).\")\n",
    "\n",
    "# Files\n",
    "CLEAN_FILE = Path(\"2504_IMF_WOO.cleaned.txt\")\n",
    "PAGE_MAP   = Path(\"weo25_page_map.json\")   # [{start,end,page}, …]\n",
    "\n",
    "# Chunking params\n",
    "CHUNK_TOKENS = 500\n",
    "OVERLAP      = 50\n",
    "EMB_BATCH    = 16\n",
    "UPL_BATCH    = 100\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# ── 3.  Helpers ────────────────────────────────────────────────────\n",
    "def slide(tokens, size, step):\n",
    "    for i in range(0, len(tokens), step):\n",
    "        yield tokens[i : i + size]\n",
    "\n",
    "def make_chunks(text: str, meta: dict):\n",
    "    step = CHUNK_TOKENS - OVERLAP\n",
    "    for idx, win in enumerate(slide(enc.encode(text), CHUNK_TOKENS, step)):\n",
    "        yield {\n",
    "            \"id\": f\"{meta['parent']}_p{meta['page']}_c{idx:04}\",\n",
    "            \"parentId\": meta[\"parent\"],\n",
    "            \"chapter\": meta[\"chapter\"],\n",
    "            \"pageStart\": meta[\"page\"],\n",
    "            \"pageEnd\": meta[\"page\"],\n",
    "            \"raw\": enc.decode(win),\n",
    "            \"@search.action\": \"upload\",\n",
    "        }\n",
    "\n",
    "# ── 4.  Load cleaned text & page map ──────────────────────────────\n",
    "if not (CLEAN_FILE.exists() and PAGE_MAP.exists()):\n",
    "    sys.exit(\"❌  Clean file or page map JSON missing\")\n",
    "\n",
    "full_text = CLEAN_FILE.read_text(\"utf-8\")\n",
    "page_map  = json.loads(PAGE_MAP.read_text())\n",
    "\n",
    "log.info(\"Splitting document on headings …\")\n",
    "head_pat = re.compile(r\"\\n([A-Z][^\\n]{3,100})\\n\")\n",
    "blocks   = head_pat.split(full_text)\n",
    "\n",
    "chunks = []\n",
    "for i in range(0, len(blocks), 2):\n",
    "    heading = blocks[i - 1].strip() if i else \"Preamble\"\n",
    "    body    = blocks[i]\n",
    "    start   = full_text.find(body)\n",
    "    page    = next(p[\"page\"] for p in page_map\n",
    "                   if p[\"start\"] <= start < p[\"end\"])\n",
    "    meta    = {\"parent\": \"WEO25\", \"chapter\": heading, \"page\": page}\n",
    "    chunks.extend(make_chunks(body, meta))\n",
    "\n",
    "log.info(\"Generated %s chunks (≈%s tokens ea).\", len(chunks), CHUNK_TOKENS)\n",
    "\n",
    "# ── 5.  Azure OpenAI embed ────────────────────────────────────────\n",
    "openai_client = openai.AzureOpenAI(\n",
    "    api_key     = AOAI_KEY,\n",
    "    api_version = AOAI_API_VERSION,\n",
    "    base_url    = f\"{AOAI_ENDPOINT}/openai/deployments/{EMBED_DEPLOYMENT}\",\n",
    ")\n",
    "\n",
    "log.info(\"Embedding with %s …\", EMBED_DEPLOYMENT)\n",
    "for i in tqdm(range(0, len(chunks), EMB_BATCH), desc=\"Embedding\", unit=\"chunk\"):\n",
    "    batch  = chunks[i : i + EMB_BATCH]\n",
    "    inputs = [c[\"raw\"] for c in batch]\n",
    "    resp   = openai_client.embeddings.create(model=EMBED_DEPLOYMENT, input=inputs)\n",
    "    for rec, emb in zip(batch, resp.data):\n",
    "        rec[\"rawVector\"] = emb.embedding\n",
    "\n",
    "# ── 6.  Upload to Azure Search ────────────────────────────────────\n",
    "search = SearchClient(\n",
    "    endpoint    = SEARCH_ENDPOINT,\n",
    "    index_name  = SEARCH_INDEX_NAME,\n",
    "    api_version = \"2024-07-01\",\n",
    "    credential  = AzureKeyCredential(SEARCH_ADMIN_KEY),\n",
    ")\n",
    "\n",
    "log.info(\"Uploading to Search index %s …\", SEARCH_INDEX_NAME)\n",
    "for i in tqdm(range(0, len(chunks), UPL_BATCH), desc=\"Uploading\", unit=\"chunk\"):\n",
    "    batch   = chunks[i : i + UPL_BATCH]\n",
    "    results = search.upload_documents(batch)\n",
    "    failed  = [r for r in results if not r.succeeded]\n",
    "    if failed:\n",
    "        log.warning(\"%d failures starting at ID %s\", len(failed), batch[0][\"id\"])\n",
    "\n",
    "log.info(\"✅  Finished — %s chunks embedded & indexed.\", len(chunks))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
