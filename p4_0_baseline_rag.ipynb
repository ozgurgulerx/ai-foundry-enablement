{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbf67af",
   "metadata": {},
   "source": [
    "### 1.0 Create an Azure AI Search Index \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08b01e",
   "metadata": {},
   "source": [
    " #### Vector-Only Index Creation with Azure AI Search\n",
    "\n",
    "This script creates a vector-only index in Azure AI Search using the General Availability (GA) schema introduced in mid-2024. It sets up an index with just two fields:\n",
    "\n",
    "A string-based document ID (used as the primary key)\n",
    "A vector field (contentVector) that holds embedding data (e.g.Azure OpenAI)\n",
    "We configure the vector search behavior to use the HNSW algorithm with cosine similarity, which is ideal for semantic search scenarios. This vector-only setup is lean and optimized for scenarios where we rely purely on vector search (e.g., similarity search in embeddings) rather than keyword-based retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a5d7199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:10:51 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')?api-version=REDACTED'\n",
      "Request method: 'PUT'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '688'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Prefer': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=minimal'\n",
      "    'x-ms-client-request-id': '048a38d2-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating or updating index 'index01' …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:10:51 [INFO] Response status: 201\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=minimal; odata.streaming=true; charset=utf-8'\n",
      "    'ETag': '\"0x8DD9EECE94D3EE1\"'\n",
      "    'Location': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '048a38d2-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Thu, 29 May 2025 20:10:52 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Index ready – text + vector fields provisioned\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "create_index_text_and_vector.py\n",
    "───────────────────────────────\n",
    "Creates/updates an Azure AI Search index that stores BOTH:\n",
    "\n",
    "• a searchable **raw** text field (full chunk text)  \n",
    "• a 1 536-d **contentVector** field for HNSW-cosine vector search\n",
    "\n",
    "Everything else is unchanged from the original script—only the extra\n",
    "`raw` field is added so query results can include readable snippets.\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "\n",
    "# ── 1. env ──────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "ENDPOINT   = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "ADMIN_KEY  = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"index01\")\n",
    "\n",
    "# ── 2. algorithm + profile (HNSW + cosine) ─────────────────────────\n",
    "algo_cfg = HnswAlgorithmConfiguration(\n",
    "    name=\"hnsw-cosine\",\n",
    "    parameters=HnswParameters(metric=\"cosine\")\n",
    ")\n",
    "\n",
    "profile_cfg = VectorSearchProfile(\n",
    "    name=\"hnsw-cosine-profile\",\n",
    "    algorithm_configuration_name=\"hnsw-cosine\",\n",
    ")\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[algo_cfg],\n",
    "    profiles=[profile_cfg],\n",
    ")\n",
    "\n",
    "# ── 3. schema: id + raw text + vector ──────────────────────────────\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "\n",
    "    # NEW: store the chunk’s plain text so we can preview it in results\n",
    "    SearchField(\n",
    "        name=\"raw\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,         # full-text search enabled\n",
    "        filterable=False,\n",
    "        facetable=False,\n",
    "        sortable=False\n",
    "    ),\n",
    "\n",
    "    SearchField(\n",
    "        name=\"contentVector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1536,\n",
    "        vector_search_profile_name=\"hnsw-cosine-profile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=INDEX_NAME,\n",
    "    fields=fields,\n",
    "    vector_search=vector_search,\n",
    ")\n",
    "\n",
    "# ── 4. push index ──────────────────────────────────────────────────\n",
    "client = SearchIndexClient(ENDPOINT, AzureKeyCredential(ADMIN_KEY))\n",
    "print(f\"Creating or updating index '{INDEX_NAME}' …\")\n",
    "client.create_or_update_index(index)\n",
    "print(\"✅  Index ready – text + vector fields provisioned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43add243",
   "metadata": {},
   "source": [
    "✅ Result\n",
    "Once this script runs, you’ll have a minimal, production-ready vector-only index that is compatible with the new GA schema and supports efficient vector similarity search via HNSW and cosine distance.\n",
    "\n",
    "You can now upload vectorized documents and perform semantic search queries efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfecb4fe",
   "metadata": {},
   "source": [
    "### 2.0 OCR the PDF \n",
    "\n",
    "This code performs OCR on a single PDF file, 2504_IMF_WOO.pdf, using Azure AI Document Intelligence and saves the extracted text as 2504_IMF_WOO.txt in the same directory. It loads API credentials from a .env file, sets up the client, and handles both script and notebook environments by resolving the working directory accordingly. The script submits the PDF to Azure’s prebuilt-read model, waits for the result, extracts text line-by-line from each page, and writes the output as a plain-text file. It includes basic error handling and status messages, making it a clean and reusable OCR workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53afacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍  Processing 2504_IMF_WOO.pdf …\n",
      "✅  Text saved to 2504_IMF_WOO.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "OCR one PDF (2504_IMF_WOO.pdf) with Azure Document Intelligence\n",
    "Saves 2504_IMF_WOO.txt in the same folder.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "\n",
    "# ─────────────── SETUP ───────────────\n",
    "try:\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent   # works in a .py file\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd()                        # Jupyter fallback\n",
    "\n",
    "load_dotenv(dotenv_path=SCRIPT_DIR / \".env\")       # credentials in .env\n",
    "\n",
    "ENDPOINT = os.getenv(\"DOCUMENTINTELLIGENCE_ENDPOINT\")\n",
    "KEY      = os.getenv(\"DOCUMENTINTELLIGENCE_API_KEY\")\n",
    "if not ENDPOINT or not KEY:\n",
    "    sys.exit(\"❌  Missing DOCUMENTINTELLIGENCE_… values in .env\")\n",
    "\n",
    "client = DocumentIntelligenceClient(\n",
    "    endpoint=ENDPOINT, credential=AzureKeyCredential(KEY)\n",
    ")\n",
    "\n",
    "PDF_FILE = SCRIPT_DIR / \"2504_IMF_WOO.pdf\"\n",
    "if not PDF_FILE.exists():\n",
    "    sys.exit(f\"❗  {PDF_FILE.name} not found in {SCRIPT_DIR.resolve()}\")\n",
    "\n",
    "print(f\"🔍  Processing {PDF_FILE.name} …\")\n",
    "\n",
    "# ─────────────── OCR ───────────────\n",
    "try:\n",
    "    with PDF_FILE.open(\"rb\") as fh:\n",
    "        poller = client.begin_analyze_document(\n",
    "            \"prebuilt-read\", fh, content_type=\"application/pdf\"\n",
    "        )\n",
    "    result = poller.result()\n",
    "\n",
    "    pages_txt = [\n",
    "        \"\\n\".join(ln.content for ln in (p.lines or []))\n",
    "        for p in (result.pages or [])\n",
    "    ]\n",
    "    (PDF_FILE.with_suffix(\".txt\")).write_text(\"\\n\\n\".join(pages_txt), \"utf-8\")\n",
    "    print(f\"✅  Text saved to {PDF_FILE.with_suffix('.txt').name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Failed to process {PDF_FILE.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a85f54",
   "metadata": {},
   "source": [
    "### 3. pre-Processing Text \n",
    "\n",
    "To prepare the OCR dump 2504_IMF_WOO.txt for RAG, the script first joins words split across line-break hyphens (e.g., “eco- \\n nomic” → “economic”). It then strips generic noise—tabs, HTML/Markdown tags, non-UTF8 bytes, divider lines, and bold “IMPORTANT/NOTE” blocks—using regex replacements. Next, it removes IMF-specific clutter such as page headers/footers, Roman- or Arabic-numbered page numbers, table-of-contents lines, chapter titles, and figure/table captions. Finally, it replaces all remaining newlines with spaces and collapses multiple spaces to one, producing a compact, boilerplate-free string that is ideal for tokenization and chunking. The cleaned output is saved as 2504_IMF_WOO.cleaned.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e029b6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Saved cleaned text → 2504_IMF_WOO.cleaned.txt  (624,877 characters)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "clean_2504_imf_woo.py  –  Create a RAG-ready version of 2504_IMF_WOO.txt\n",
    "\n",
    "Reads the raw OCR dump, removes headers/footers, TOC noise, figure captions,\n",
    "hyphen-breaks, HTML/Markdown tags, non-UTF8 chars, etc., and writes\n",
    "2504_IMF_WOO.cleaned.txt in the same directory.\n",
    "\n",
    "Run with:  python clean_2504_imf_woo.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1.  Text-cleaning utility\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Return a compact, boilerplate-free string suitable for chunking.\"\"\"\n",
    "    # fix hyphenated line breaks  (eco-\\n  nomic → economic)\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "\n",
    "    # generic noise\n",
    "    generic = [\n",
    "        r\"\\t\", r\"\\r\\n\", r\"\\r\",                # tabs / CRs\n",
    "        r\"[^\\x00-\\x7F]+\",                     # non-UTF8\n",
    "        r\"<\\/?(table|tr|td|ul|li|p|br)>\",     # HTML tags\n",
    "        r\"\\*\\*IMPORTANT:\\*\\*|\\*\\*NOTE:\\*\\*\", # doc notes\n",
    "        r\"<!|no-loc |text=|<--|-->\",          # markup\n",
    "        r\"```|:::|---|--|###|##|#\",           # md code / hr / headers\n",
    "    ]\n",
    "    for pat in generic:\n",
    "        text = re.sub(pat, \" \", text, flags=re.I)\n",
    "\n",
    "    # IMF-specific headers / footers / TOC lines / captions\n",
    "    imf_noise = [\n",
    "        r\"INTERNATIONAL MONETARY FUND\",\n",
    "        r\"WORLD\\s+ECONOMIC\\s+OUTLOOK\",\n",
    "        r\"\\|\\s*April\\s+\\d{4}\",\n",
    "        r\"^CONTENTS$|^DATA$|^PREFACE$|^FOREWORD$|^EXECUTIVE SUMMARY$\",\n",
    "        r\"^ASSUMPTIONS AND CONVENTIONS$|^FURTHER INFORMATION$|^ERRATA$\",\n",
    "        r\"^Chapter\\s+\\d+.*$\",\n",
    "        r\"^(Table|Figure|Box|Annex)\\s+[A-Z0-9].*$\",\n",
    "        r\"^\\s*[ivxlcdm]+\\s*$\",   # Roman numerals\n",
    "        r\"^\\s*\\d+\\s*$\",          # arabic page nos\n",
    "    ]\n",
    "    for pat in imf_noise:\n",
    "        text = re.sub(pat, \" \", text, flags=re.I | re.M)\n",
    "\n",
    "    # remove remaining newlines → single spaces\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2.  Entrypoint\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def main() -> None:\n",
    "    raw_path = Path.cwd() / \"2504_IMF_WOO.txt\"\n",
    "    if not raw_path.exists():\n",
    "        sys.exit(f\"❌  {raw_path.name} not found in {Path.cwd()}\")\n",
    "\n",
    "    raw_text = raw_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    cleaned   = clean_text(raw_text)\n",
    "\n",
    "    out_path = raw_path.with_suffix(\".cleaned.txt\")\n",
    "    out_path.write_text(cleaned, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"✅  Saved cleaned text → {out_path.name}  \"\n",
    "          f\"({len(cleaned):,} characters)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185a5da",
   "metadata": {},
   "source": [
    "### 4. Chunking Documents for RAG\n",
    "\n",
    "### 🔹 What Is “Chunking” in RAG?\n",
    "\n",
    "*Chunking* means slicing long documents into smaller, self-contained pieces (“chunks”) so they fit the model’s token window and can be embedded, indexed, and retrieved accurately. The aim is to keep **enough context** for a useful answer while **avoiding overly large inputs** that waste tokens or hurt precision.\n",
    "\n",
    "---\n",
    "\n",
    "#### Common Chunking Methods\n",
    "\n",
    "| Method | How it works | Best for |\n",
    "|--------|--------------|----------|\n",
    "| **Fixed-length windows** | Split every *N* tokens/characters, often with 10–20 % overlap. | Logs, code, data dumps where structure ≈ length. |\n",
    "| **Sentence/paragraph split** | Use an NLP splitter; keep full sentences or paragraphs. | Narrative or news text; avoids mid-sentence cuts. |\n",
    "| **Recursive / semantic split** | Split on headings → paragraphs → sentences until each piece < limit (e.g., LangChain `RecursiveCharacterTextSplitter`). | Long structured docs (white papers, legal contracts). |\n",
    "| **Sliding window at retrieval** | No pre-processing; generate overlapping windows on demand around query anchors. | Recall-critical QA (wikis, forums) when storage is cheap. |\n",
    "| **Adaptive / LLM-assisted** | An LLM places boundaries where topics shift. | Highly variable content; experimental but coherent. |\n",
    "\n",
    "---\n",
    "\n",
    "#### Choosing a Strategy\n",
    "\n",
    "* **Code & logs:** fixed 400-token windows + 10 % overlap.  \n",
    "* **Technical reports / legal PDFs:** recursive splitting on headings.  \n",
    "* **Emails & web articles:** paragraph/sentence chunks of ~300-500 tokens.  \n",
    "* **Large wiki corpora:** sliding windows to maximise recall.  \n",
    "* **Mixed formats needing topic coherence:** try LLM-assisted splitting.\n",
    "\n",
    "> **Rule of thumb:** keep chunks **200–800 tokens** and add a small overlap when continuity matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "996150eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:14:08 [INFO] Splitting document on headings …\n",
      "23:14:08 [INFO] Generated 414 chunks (≈500 tokens each).\n",
      "23:14:08 [INFO] Embedding with text-embedding-3-small …\n",
      "Embedding:   0%|          | 0/26 [00:00<?, ?chunk/s]23:14:09 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:   4%|▍         | 1/26 [00:01<00:27,  1.10s/chunk]23:14:09 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:   8%|▊         | 2/26 [00:01<00:16,  1.44chunk/s]23:14:10 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  12%|█▏        | 3/26 [00:01<00:12,  1.82chunk/s]23:14:10 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  15%|█▌        | 4/26 [00:02<00:10,  2.15chunk/s]23:14:11 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  19%|█▉        | 5/26 [00:02<00:09,  2.24chunk/s]23:14:11 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  23%|██▎       | 6/26 [00:03<00:08,  2.30chunk/s]23:14:11 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  27%|██▋       | 7/26 [00:03<00:08,  2.35chunk/s]23:14:12 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  31%|███       | 8/26 [00:03<00:06,  2.57chunk/s]23:14:12 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  35%|███▍      | 9/26 [00:04<00:06,  2.64chunk/s]23:14:12 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  38%|███▊      | 10/26 [00:04<00:05,  2.70chunk/s]23:14:13 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  42%|████▏     | 11/26 [00:04<00:05,  2.74chunk/s]23:14:13 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  46%|████▌     | 12/26 [00:05<00:05,  2.80chunk/s]23:14:13 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  50%|█████     | 13/26 [00:05<00:04,  2.87chunk/s]23:14:14 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  54%|█████▍    | 14/26 [00:05<00:04,  2.56chunk/s]23:14:14 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  58%|█████▊    | 15/26 [00:06<00:04,  2.39chunk/s]23:14:15 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  62%|██████▏   | 16/26 [00:06<00:04,  2.48chunk/s]23:14:15 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  65%|██████▌   | 17/26 [00:07<00:03,  2.67chunk/s]23:14:15 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  69%|██████▉   | 18/26 [00:07<00:02,  2.79chunk/s]23:14:16 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  73%|███████▎  | 19/26 [00:07<00:02,  2.60chunk/s]23:14:16 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  77%|███████▋  | 20/26 [00:08<00:02,  2.75chunk/s]23:14:16 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  81%|████████  | 21/26 [00:08<00:01,  2.84chunk/s]23:14:17 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  85%|████████▍ | 22/26 [00:08<00:01,  2.64chunk/s]23:14:17 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  88%|████████▊ | 23/26 [00:09<00:01,  2.78chunk/s]23:14:17 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  92%|█████████▏| 24/26 [00:09<00:00,  2.90chunk/s]23:14:18 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding:  96%|█████████▌| 25/26 [00:09<00:00,  2.98chunk/s]23:14:18 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "Embedding: 100%|██████████| 26/26 [00:10<00:00,  2.54chunk/s]\n",
      "23:14:18 [INFO] Uploading to Search index index01 …\n",
      "Uploading:   0%|          | 0/5 [00:00<?, ?chunk/s]23:14:18 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '3643993'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': '8052153e-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "23:14:20 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '8052153e-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Thu, 29 May 2025 20:14:20 GMT'\n",
      "Uploading:  20%|██        | 1/5 [00:01<00:07,  1.76s/chunk]23:14:20 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '3645284'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': '815d7464-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "23:14:22 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '815d7464-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Thu, 29 May 2025 20:14:22 GMT'\n",
      "Uploading:  40%|████      | 2/5 [00:03<00:05,  1.69s/chunk]23:14:22 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '3571518'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': '8255f47c-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "23:14:23 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '8255f47c-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Thu, 29 May 2025 20:14:23 GMT'\n",
      "Uploading:  60%|██████    | 3/5 [00:04<00:03,  1.62s/chunk]23:14:23 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '3512775'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': '83424e8a-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "23:14:24 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '83424e8a-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Thu, 29 May 2025 20:14:24 GMT'\n",
      "Uploading:  80%|████████  | 4/5 [00:06<00:01,  1.47s/chunk]23:14:24 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.index?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '509866'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': '83f7dd72-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "23:14:25 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '83f7dd72-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Thu, 29 May 2025 20:14:24 GMT'\n",
      "Uploading: 100%|██████████| 5/5 [00:06<00:00,  1.35s/chunk]\n",
      "23:14:25 [INFO] ✅  Done — 414 chunks embedded & indexed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "embed_and_upload_chunks.py\n",
    "──────────────────────────\n",
    "• Reads IMF WEO cleaned text (2504_IMF_WOO.cleaned.txt)\n",
    "• Splits it into ≈ 500-token chunks (10 % overlap, heading-aware)\n",
    "• Embeds each chunk with Azure OpenAI\n",
    "• **Uploads id + raw + contentVector** to Azure AI Search index01\n",
    "• Shows INFO logging and tqdm progress bars\n",
    "\n",
    "(raw text is now retained so query results can include readable snippets)\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, re, logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tiktoken\n",
    "import openai\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "# ── 1. logging ────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"IMF-Embed\")\n",
    "\n",
    "# ── 2. env vars ───────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "\n",
    "SEARCH_ENDPOINT   = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "SEARCH_ADMIN_KEY  = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"index01\")\n",
    "if not SEARCH_ENDPOINT or not SEARCH_ADMIN_KEY:\n",
    "    sys.exit(\"❌  Missing AZURE_SEARCH_* vars in .env\")\n",
    "\n",
    "AOAI_ENDPOINT     = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\").rstrip(\"/\")\n",
    "AOAI_KEY          = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AOAI_API_VERSION  = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "EMBED_DEPLOYMENT  = os.getenv(\"AZURE_TEXT_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "if not AOAI_ENDPOINT or not AOAI_KEY or not EMBED_DEPLOYMENT:\n",
    "    sys.exit(\"❌  Missing Azure OpenAI vars in .env\")\n",
    "\n",
    "# ── 3. files & params ────────────────────────────────────────────\n",
    "CLEAN_FILE   = Path(\"2504_IMF_WOO.cleaned.txt\")\n",
    "CHUNK_TOKENS = 500\n",
    "OVERLAP      = 50\n",
    "EMB_BATCH    = 16\n",
    "UPL_BATCH    = 100\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# ── 4. helpers ────────────────────────────────────────────────────\n",
    "def slide(tokens, size, step):\n",
    "    for i in range(0, len(tokens), step):\n",
    "        yield tokens[i : i + size]\n",
    "\n",
    "def chunkify(text: str, parent: str = \"WEO25\"):\n",
    "    step = CHUNK_TOKENS - OVERLAP\n",
    "    tokens = enc.encode(text)\n",
    "    for idx, win in enumerate(slide(tokens, CHUNK_TOKENS, step)):\n",
    "        yield {\n",
    "            \"id\": f\"{parent}_c{idx:06}\",\n",
    "            \"raw\": enc.decode(win),          # kept for query-time snippets\n",
    "            \"@search.action\": \"upload\",\n",
    "        }\n",
    "\n",
    "# ── 5. load cleaned text ─────────────────────────────────────────\n",
    "if not CLEAN_FILE.exists():\n",
    "    sys.exit(\"❌  Cleaned text file not found\")\n",
    "\n",
    "full_text = CLEAN_FILE.read_text(\"utf-8\")\n",
    "\n",
    "log.info(\"Splitting document on headings …\")\n",
    "blocks = re.split(r\"\\n([A-Z][^\\n]{3,100})\\n\", full_text)  # even=text, odd=heading\n",
    "\n",
    "chunks = []\n",
    "for i in range(0, len(blocks), 2):\n",
    "    body = blocks[i]\n",
    "    chunks.extend(chunkify(body))\n",
    "\n",
    "log.info(\"Generated %s chunks (≈%s tokens each).\", len(chunks), CHUNK_TOKENS)\n",
    "\n",
    "# ── 6. embed ─────────────────────────────────────────────────────\n",
    "openai_client = openai.AzureOpenAI(\n",
    "    api_key     = AOAI_KEY,\n",
    "    api_version = AOAI_API_VERSION,\n",
    "    base_url    = f\"{AOAI_ENDPOINT}/openai/deployments/{EMBED_DEPLOYMENT}\",\n",
    ")\n",
    "\n",
    "log.info(\"Embedding with %s …\", EMBED_DEPLOYMENT)\n",
    "for i in tqdm(range(0, len(chunks), EMB_BATCH), desc=\"Embedding\", unit=\"chunk\"):\n",
    "    batch  = chunks[i:i+EMB_BATCH]\n",
    "    inputs = [c[\"raw\"] for c in batch]\n",
    "    resp   = openai_client.embeddings.create(model=EMBED_DEPLOYMENT, input=inputs)\n",
    "    for rec, emb in zip(batch, resp.data):\n",
    "        rec[\"contentVector\"] = emb.embedding  # name matches index field\n",
    "\n",
    "# ── 7. upload ────────────────────────────────────────────────────\n",
    "search = SearchClient(\n",
    "    endpoint    = SEARCH_ENDPOINT,\n",
    "    index_name  = SEARCH_INDEX_NAME,\n",
    "    api_version = \"2024-07-01\",\n",
    "    credential  = AzureKeyCredential(SEARCH_ADMIN_KEY),\n",
    ")\n",
    "\n",
    "log.info(\"Uploading to Search index %s …\", SEARCH_INDEX_NAME)\n",
    "for i in tqdm(range(0, len(chunks), UPL_BATCH), desc=\"Uploading\", unit=\"chunk\"):\n",
    "    batch   = chunks[i:i+UPL_BATCH]\n",
    "    results = search.upload_documents(batch)\n",
    "    fails   = [r for r in results if not r.succeeded]\n",
    "    if fails:\n",
    "        log.warning(\"%d failures (starting at %s)\", len(fails), batch[0][\"id\"])\n",
    "\n",
    "log.info(\"✅  Done — %s chunks embedded & indexed.\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0456319",
   "metadata": {},
   "source": [
    "This script automates the pipeline for turning the **cleaned IMF WEO report** into a searchable vector index. It loads *2504_IMF_WOO.cleaned.txt*, splits the text into ~500-token chunks with a 10 % overlap (to avoid cutting important context), and sends each chunk to Azure OpenAI’s **text-embedding-3-small** model to obtain a 1 536-dimensional vector. After embedding, the script uploads only two fields—`id` and `contentVector`—to the Azure AI Search index **index01**, which is set up as a vector-only schema. All credentials come from the `.env` file, and `tqdm` progress bars plus INFO-level logging make it easy to track embedding and upload progress inside a Jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de288b90",
   "metadata": {},
   "source": [
    "### 5. Query the Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8836e90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:14:32 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "23:14:32 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.post.search?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '34274'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': '88c2f38c-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "23:14:33 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': '88c2f38c-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Thu, 29 May 2025 20:14:33 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WEO25_c000087  (score 0.765)\n",
      "(2030e) Sources: International Energy Agency (IEA); Organization of the Petroleum Exporting Countries (OPEC); and IMF staff calculations. Note: Estimates for data centers (DCs) and electric vehicles (EVs) are for the world and come from OPEC and the IEA, respectively. Data labels in the figure use\n",
      "\n",
      "WEO25_c000088  (score 0.759)\n",
      "greater use of compute by companies pursuing better-performing models (Hoffmann and others 2022). Adding to this complexity is the recent emergence of reasoning models-which require more compute in their deployment-and possibly greater AI use driven by lower costs and availability of open-source mo\n",
      "\n",
      "WEO25_c000089  (score 0.741)\n",
      "4). 8 percent in the United States (525 TWh), 3 percent in Europe (145 TWh), and 2 percent in China (237 TWh) relative to the baseline scenario. In the AI scenario under alternative energy policies, the increase in total electricity supply is kept the same, but its composition shifts in favor of ren\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vector_query.py\n",
    "────────────────\n",
    "Embed a natural-language question, run a vector search against index01,\n",
    "and print the top-k chunk IDs, scores, and (if present) a text preview.\n",
    "All credentials come from .env.\n",
    "\"\"\"\n",
    "\n",
    "import os, openai, textwrap\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# ── config ────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "\n",
    "QUESTION = \"How will AI affect future energy demand according to the report?\"\n",
    "TOP_K    = 3                                     # number of hits to return\n",
    "VECTOR_FIELD = \"contentVector\"                   # name in your index\n",
    "\n",
    "# ── embed the question ───────────────────────────────────\n",
    "aoai = openai.AzureOpenAI(\n",
    "    api_key     = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    base_url    = f\"{os.getenv('AZURE_OPENAI_ENDPOINT').rstrip('/')}/openai/deployments/{os.getenv('AZURE_TEXT_EMBEDDING_DEPLOYMENT_NAME')}\"\n",
    ")\n",
    "embedding = aoai.embeddings.create(\n",
    "    model=os.getenv(\"AZURE_TEXT_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    input=[QUESTION]\n",
    ").data[0].embedding\n",
    "\n",
    "# ── build & run vector search ─────────────────────────────\n",
    "vector_query = VectorizedQuery(vector=embedding, fields=VECTOR_FIELD)\n",
    "\n",
    "search = SearchClient(\n",
    "    endpoint   = os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"index01\"),\n",
    "    credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")),\n",
    "    api_version=\"2024-07-01\"\n",
    ")\n",
    "\n",
    "results = search.search(\n",
    "    search_text=\"\",                 # blank keyword query\n",
    "    vector_queries=[vector_query],\n",
    "    top=TOP_K                       # number of nearest neighbours\n",
    ")\n",
    "\n",
    "# ── display hits ──────────────────────────────────────────\n",
    "for doc in results:\n",
    "    score = doc['@search.score']\n",
    "    snippet = doc.get(\"raw\", \"\")[:300].replace(\"\\n\", \" \")\n",
    "    print(f\"\\n{doc['id']}  (score {score:.3f})\")\n",
    "    print(textwrap.shorten(snippet, 300) if snippet else \"(raw text not stored)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d7149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:17:14 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "23:17:14 [WARNING] k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizedQuery'> and will be ignored\n",
      "23:17:14 [INFO] Request URL: 'https://chatops-ozguler.search.windows.net/indexes('index01')/docs/search.post.search?api-version=REDACTED'\n",
      "Request method: 'POST'\n",
      "Request headers:\n",
      "    'Content-Type': 'application/json'\n",
      "    'Content-Length': '34274'\n",
      "    'api-key': 'REDACTED'\n",
      "    'Accept': 'application/json;odata.metadata=none'\n",
      "    'x-ms-client-request-id': 'e90fd07a-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'User-Agent': 'azsdk-python-search-documents/11.5.2 Python/3.12.10 (macOS-15.5-arm64-arm-64bit)'\n",
      "A body is sent with the request\n",
      "23:17:14 [INFO] Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json; odata.metadata=none; odata.streaming=true; charset=utf-8'\n",
      "    'Content-Encoding': 'REDACTED'\n",
      "    'Vary': 'REDACTED'\n",
      "    'Server': 'Microsoft-IIS/10.0'\n",
      "    'Strict-Transport-Security': 'REDACTED'\n",
      "    'Preference-Applied': 'REDACTED'\n",
      "    'OData-Version': 'REDACTED'\n",
      "    'request-id': 'e90fd07a-3cc9-11f0-9ff2-4eb2cec3a125'\n",
      "    'elapsed-time': 'REDACTED'\n",
      "    'Date': 'Thu, 29 May 2025 20:17:14 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WEO25_c000087  (score 0.765)\n",
      "(2030e) Sources: International Energy Agency (IEA); Organization of the Petroleum Exporting Countries (OPEC); and IMF staff calculations. Note: Estimates for data centers (DCs) and electric vehicles (EVs) are for the world and come from OPEC and the IEA, respectively. Data labels in the figure use\n",
      "\n",
      "WEO25_c000088  (score 0.759)\n",
      "greater use of compute by companies pursuing better-performing models (Hoffmann and others 2022). Adding to this complexity is the recent emergence of reasoning models-which require more compute in their deployment-and possibly greater AI use driven by lower costs and availability of open-source mo\n",
      "\n",
      "WEO25_c000089  (score 0.741)\n",
      "4). 8 percent in the United States (525 TWh), 3 percent in Europe (145 TWh), and 2 percent in China (237 TWh) relative to the baseline scenario. In the AI scenario under alternative energy policies, the increase in total electricity supply is kept the same, but its composition shifts in favor of ren\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:17:15 [INFO] HTTP Request: POST https://aoai-ep-swedencentral02.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 400 Bad Request\"\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     62\u001b[39m chat = openai.AzureOpenAI(\n\u001b[32m     63\u001b[39m     api_key=AOAI_KEY,\n\u001b[32m     64\u001b[39m     api_version=AOAI_VER,\n\u001b[32m     65\u001b[39m     base_url=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAOAI_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/openai/deployments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_DEPLOY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m )\n\u001b[32m     68\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an analyst answering questions using the provided report excerpts.\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[33mAnswer concisely and cite excerpts by chunk id when relevant.\u001b[39m\n\u001b[32m     70\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[33mAnswer:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = \u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHAT_DEPLOY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\n\u001b[32m     84\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m── LLM Answer ──\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Projects/ai-foundry-enablement/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Projects/ai-foundry-enablement/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Projects/ai-foundry-enablement/venv/lib/python3.12/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Projects/ai-foundry-enablement/venv/lib/python3.12/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "rag_query.py\n",
    "────────────\n",
    "1. Embed a natural-language question\n",
    "2. Retrieve the TOP_K most similar chunks from Azure AI Search\n",
    "3. Feed those chunks to a chat-capable Azure OpenAI model\n",
    "4. Print the LLM’s grounded answer + show which chunks were used\n",
    "\"\"\"\n",
    "\n",
    "import os, textwrap, openai\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# ── 0. config ──────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "\n",
    "QUESTION     = \"How will AI affect future energy demand according to the report?\"\n",
    "TOP_K        = 3\n",
    "VECTOR_FIELD = \"contentVector\"\n",
    "\n",
    "EMBED_DEPLOY = os.getenv(\"AZURE_TEXT_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "CHAT_DEPLOY  = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\",\n",
    "                         os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"))  # fallback\n",
    "AOAI_BASE    = os.getenv(\"AZURE_OPENAI_ENDPOINT\").rstrip(\"/\")\n",
    "AOAI_KEY     = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AOAI_VER     = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "# ── 1. embed the question ─────────────────────────────────────────\n",
    "aoai = openai.AzureOpenAI(\n",
    "    api_key=AOAI_KEY,\n",
    "    api_version=AOAI_VER,\n",
    "    base_url=f\"{AOAI_BASE}/openai/deployments/{EMBED_DEPLOY}\"\n",
    ")\n",
    "embedding = aoai.embeddings.create(model=EMBED_DEPLOY, input=[QUESTION]).data[0].embedding\n",
    "\n",
    "# ── 2. vector search ──────────────────────────────────────────────\n",
    "search = SearchClient(\n",
    "    endpoint   = os.getenv(\"AZURE_SEARCH_ENDPOINT\"),\n",
    "    index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\", \"index01\"),\n",
    "    credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")),\n",
    "    api_version=\"2024-07-01\"\n",
    ")\n",
    "\n",
    "vector_query = VectorizedQuery(vector=embedding, fields=VECTOR_FIELD, k=TOP_K)\n",
    "hits = list(search.search(search_text=\"\", vector_queries=[vector_query], top=TOP_K))\n",
    "\n",
    "contexts = []\n",
    "for h in hits:\n",
    "    snippet = (h.get(\"raw\") or \"\")[:300].replace(\"\\n\", \" \")\n",
    "    contexts.append(h.get(\"raw\", \"\"))\n",
    "    print(f\"\\n{h['id']}  (score {h['@search.score']:.3f})\")\n",
    "    print(textwrap.shorten(snippet, 300) if snippet else \"(raw text not stored)\")\n",
    "\n",
    "if not contexts:\n",
    "    print(\"\\nNo chunks retrieved – cannot answer.\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ── 3. ask the LLM with retrieved context ─────────────────────────\n",
    "chat = openai.AzureOpenAI(\n",
    "    api_key=AOAI_KEY,\n",
    "    api_version=AOAI_VER,\n",
    "    base_url=f\"{AOAI_BASE}/openai/deployments/{CHAT_DEPLOY}\"\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"You are an analyst answering questions using the provided report excerpts.\n",
    "Answer concisely and cite excerpts by chunk id when relevant.\n",
    "\n",
    "Question:\n",
    "{QUESTION}\n",
    "\n",
    "Excerpts:\n",
    "{\"\\n\\n\".join(f\"[{hits[i]['id']}]\\n{contexts[i]}\" for i in range(len(contexts)))}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = chat.chat.completions.create(\n",
    "    model=CHAT_DEPLOY,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=400,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"\\n── LLM Answer ──\\n\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
