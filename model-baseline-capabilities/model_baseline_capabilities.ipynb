{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beba34d0",
   "metadata": {},
   "source": [
    "## AI Foundry Workshop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b3f14e",
   "metadata": {},
   "source": [
    "### 1. Completions API \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f256f",
   "metadata": {},
   "source": [
    "#### 1.1 Completions API - gpt-4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ceda8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black hole is a region in space where gravity is so strong that nothing—not even light—can escape from it. It forms when a massive star runs out of fuel and collapses under its own gravity. The boundary around a black hole, beyond which nothing can escape, is called the \"event horizon.\" Anything that crosses this boundary is pulled into the black hole and cannot get out. Black holes can be very small or millions of times heavier than our Sun, and they are invisible because they do not emit light themselves. However, scientists can detect black holes by observing how they affect nearby stars and gas.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ── 1. env + client ───────────────────────────────────────────────────────\n",
    "load_dotenv(\".env\")                                  # AZURE_* vars live here\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version    = os.getenv(\"AZURE_OPENAI_API_VERSION\"),  # e.g. 2024-06-01-preview\n",
    ")\n",
    "DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")       # your model deployment name\n",
    "\n",
    "# ── 2. system-and-user messages ───────────────────────────────────────────\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise astrophysics tutor who explains concepts in plain English.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What is a black hole?\"}\n",
    "]\n",
    "\n",
    "# ── 3. one-shot call ──────────────────────────────────────────────────────\n",
    "response = client.chat.completions.create(\n",
    "    model       = DEPLOYMENT,\n",
    "    messages    = messages,\n",
    "    temperature = 0.7,\n",
    "    max_tokens  = 256,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cb40c",
   "metadata": {},
   "source": [
    "#### 1.2 Responses API - Reasoning models - o4-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb9c582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A black hole is a region of spacetime where gravity is so intense that nothing—not even light—can escape once it crosses a boundary called the event horizon. Key points:  \n",
      "• Formation: most commonly from the gravitational collapse of a massive star’s core after it exhausts its nuclear fuel.  \n",
      "• Event horizon: the “point of no return” marking the black hole’s boundary; its radius for a non-rotating (Schwarzschild) black hole is Rs = 2GM/c².  \n",
      "• Singularity: a (theoretical) point at the center where density and spacetime curvature become infinite under classical general relativity.  \n",
      "• Types:  \n",
      "  – Stellar-mass (a few to tens of solar masses)  \n",
      "  – Supermassive (10^6–10^10 M☉) at galactic centers  \n",
      "  – Intermediate and primordial (hypothetical)  \n",
      "• Detection: inferred via effects on nearby matter (accretion disks, X-ray emission), star-orbit dynamics, gravitational lensing and gravitational waves from mergers.  \n",
      "• No-hair theorem: a black hole in isolation is fully described by just mass, electric charge and spin (angular momentum).\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ── 1. Load env & create client ───────────────────────────────────────────\n",
    "load_dotenv(\".env\")                                    # holds your AZURE_* vars\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    # reasoning models live on a preview API version (adjust if needed)\n",
    "    api_version    = os.getenv(\"AZURE_REASONING_OPENAI_API_VERSION\") \n",
    "                 or \"2025-04-01-preview\",\n",
    ")\n",
    "\n",
    "DEPLOYMENT = os.getenv(\"AZURE_OPENAI_REASONING_DEPLOYMENT_NAME\")  \n",
    "\n",
    "# ── 2. Build the input messages ───────────────────────────────────────────\n",
    "messages = [\n",
    "    {\"role\": \"developer\", \"content\": \"You are a concise physics tutor.\"},\n",
    "    {\"role\": \"user\",      \"content\": \"What is a black hole?\"}\n",
    "]\n",
    "\n",
    "# ── 3. Single Responses-API call ──────────────────────────────────────────\n",
    "response = client.responses.create(\n",
    "    model               = DEPLOYMENT,\n",
    "    input               = messages,            # list of role/content dicts\n",
    "    reasoning           = {\"effort\": \"medium\"},# low | medium | high\n",
    "    max_output_tokens   = 1024                 # covers reasoning + visible answer\n",
    ")\n",
    "\n",
    "print(response.output_text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ceb5bc",
   "metadata": {},
   "source": [
    "#### 1.3 Completions API - model router "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0cbb164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model chosen by the router:  gpt-4.1-nano-2025-04-14\n",
      "A comprehensive Martian terraforming plan involves multiple steps:\n",
      "\n",
      "1. **Atmospheric Thickening:** Release greenhouse gases (e.g., perfluorocarbons) to trap heat, raising surface temperatures. Utilizing in-situ resources like carbon dioxide from the polar ice caps and regolith can augment atmospheric CO₂ levels.\n",
      "\n",
      "2. **Surface Warming:** Install large-scale orbital mirrors to reflect sunlight onto the surface, increasing temperature and sublimation of polar ice.\n",
      "\n",
      "3. **Water Availability:** Promote melting of polar ice caps to create liquid water sources. Introduce genetically engineered microbes to produce oxygen and stabilize the environment.\n",
      "\n",
      "4. **Materials Needed:**\n",
      "   - Greenhouse gases (perfluorocarbons, sulfur hexafluoride)\n",
      "   - Reflective orbital mirrors (composed of lightweight, durable materials like aluminum or carbon composites)\n",
      "   - Microbial life forms designed for Mars conditions\n",
      "   - Construction materials for habitats (regolith-based bricks, lightweight composites)\n",
      "   \n",
      "5. **Long-term Stabilization:** Develop a magnetic shield or artificially enhance Mars' magnetic field to protect the atmosphere from solar wind erosion.\n",
      "\n",
      "This multi-decade process aims to transform Mars into a more Earth-like environment suitable for human habitation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI      # pip install openai>=1.14.0\n",
    "\n",
    "# ── 1. Create the client ───────────────────────────────────────────────────────\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",           # required for Router + o-series\n",
    "    azure_endpoint   = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "# ── 2. Define the chat messages ────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise astrophysics tutor.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"Come up with a terraforming plan for human settlements in Mars so that atmosphere becomes earthlike and consider the type of materials ...\"}\n",
    "],\n",
    "    max_tokens=8192,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    model=\"model-router\"\n",
    ")\n",
    "\n",
    "print(\"Model chosen by the router: \", response.model)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf38c9",
   "metadata": {},
   "source": [
    "#### 1.4 COMPLETIONS API + STRUCTURED OUTPUTS \n",
    "\n",
    "In this example, we’re using a structured output feature that lets us define exactly what kind of response we want from the model—using a Pydantic class called BookBrief. Instead of hoping the model returns something that looks like JSON and then writing code to clean it up, we give the model a strict schema—three fields: author, year, and summary. The model must follow this structure, and if it doesn't, the SDK will throw an error immediately. That means no more post-processing or defensive coding—we get back a typed Python object, ready to use, with guaranteed field names and types. This is incredibly useful when you’re building apps that rely on structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e19539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author : Harper Lee\n",
      "Year   : 1960\n",
      "Summary: Set in the Depression-era South, the novel follows young Scout Finch as her father, Atticus, defends a black man falsely accused of rape, exploring themes of racism and moral growth.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "structured_outputs_demo.py\n",
    "──────────────────────────\n",
    "Extract three distinct pieces of data—in *one* call—using Azure OpenAI\n",
    "**structured outputs** (schema-enforced JSON).\n",
    "\n",
    "Result comes back as a typed Pydantic object, so you can use the fields\n",
    "directly without any manual parsing or key-checks.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ── 1. env + client ───────────────────────────────────────────────────────\n",
    "load_dotenv(\".env\")   # must contain AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version    = \"2024-10-21\",                 # GA version with structured outputs\n",
    ")\n",
    "\n",
    "DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")      # model that supports them\n",
    "\n",
    "# ── 2. define desired output schema (multiple fields) ─────────────────────\n",
    "class BookBrief(BaseModel):\n",
    "    author:   str                     = Field(..., description=\"Book’s author\")\n",
    "    year:     int                     = Field(..., description=\"Year first published (YYYY)\")\n",
    "    summary:  str                     = Field(..., description=\"≤30-word overview\")\n",
    "\n",
    "# ── 3. ask the model & let SDK return a typed object ──────────────────────\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model = DEPLOYMENT,\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Return data that matches the schema exactly.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"Give me author, publication year, and a 30-word summary of 'To Kill a Mockingbird'.\"}\n",
    "    ],\n",
    "    response_format = BookBrief,      # ← structured outputs\n",
    ")\n",
    "\n",
    "book: BookBrief = response.choices[0].message.parsed\n",
    "\n",
    "# ── 4. use the structured result ──────────────────────────────────────────\n",
    "print(\"Author :\", book.author)\n",
    "print(\"Year   :\", book.year)\n",
    "print(\"Summary:\", book.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ce0321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Model output:\n",
      " {\n",
      "  \"author\": \"Harper Lee\",\n",
      "  \"year\": 1960,\n",
      "  \"summary\": \"To Kill a Mockingbird is a classic American novel set in the 1930s in the fictional town of Maycomb, Alabama. It follows the story of young Scout Finch, her brother Jem, and their father Atticus, a principled lawyer who defends a Black man wrongly accused of raping a white woman. Through Scout's eyes, the novel explores themes of racial injustice, moral growth, empathy, and the loss of innocence.\"\n",
      "}\n",
      "\n",
      "Parsed:\n",
      " {'author': 'Harper Lee', 'year': 1960, 'summary': \"To Kill a Mockingbird is a classic American novel set in the 1930s in the fictional town of Maycomb, Alabama. It follows the story of young Scout Finch, her brother Jem, and their father Atticus, a principled lawyer who defends a Black man wrongly accused of raping a white woman. Through Scout's eyes, the novel explores themes of racial injustice, moral growth, empathy, and the loss of innocence.\"}\n",
      "\n",
      "Author: Harper Lee\n",
      "Year: 1960\n",
      "Summary: To Kill a Mockingbird is a classic American novel set in the 1930s in the fictional town of Maycomb, Alabama. It follows the story of young Scout Finch, her brother Jem, and their father Atticus, a principled lawyer who defends a Black man wrongly accused of raping a white woman. Through Scout's eyes, the novel explores themes of racial injustice, moral growth, empathy, and the loss of innocence.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "structured_outputs_failure_demo.py\n",
    "──────────────────────────────────\n",
    "Same question, but without structured outputs—just hoping the model\n",
    "returns valid, expected JSON. Watch how this can break.\n",
    "\"\"\"\n",
    "\n",
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ── 1. env + client ───────────────────────────────────────────────────────\n",
    "load_dotenv(\".env\")   # must contain AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_API_KEY\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version    = \"2024-10-21\",   # doesn't matter—no schema involved here\n",
    ")\n",
    "\n",
    "DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# ── 2. ask the model with a vague instruction ─────────────────────────────\n",
    "response = client.chat.completions.create(\n",
    "    model = DEPLOYMENT,\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Return JSON with author, year, and summary.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"Tell me about 'To Kill a Mockingbird'.\"}\n",
    "    ],\n",
    "    temperature = 0.7,\n",
    "    max_tokens  = 256,\n",
    ")\n",
    "\n",
    "raw_output = response.choices[0].message.content\n",
    "print(\"⬇️ Model output:\\n\", raw_output)\n",
    "\n",
    "# ── 3. try parsing the JSON (this might fail or produce wrong structure) ─\n",
    "try:\n",
    "    parsed = json.loads(raw_output)\n",
    "    print(\"\\nParsed:\\n\", parsed)\n",
    "    print(\"\\nAuthor:\", parsed[\"author\"])\n",
    "    print(\"Year:\", parsed[\"year\"])\n",
    "    print(\"Summary:\", parsed[\"summary\"])\n",
    "except Exception as err:\n",
    "    print(\"\\n❌ Failed to parse or access expected fields:\\n\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ebd95",
   "metadata": {},
   "source": [
    "#### 1.5 COMPLETIONS API + REPRODUCIBLE OUTPUTS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71062f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1:\n",
      "---\n",
      "Here’s a surprising fact: The moon is moving away from Earth at a rate of about **3.8 centimeters (1.5 inches) per year**! This slow drift is caused by the tidal interactions between the Earth and the moon. Over\n",
      "---\n",
      "\n",
      "Run 2:\n",
      "---\n",
      "Here’s a surprising fact: The moon is moving away from Earth at a rate of about 3.8 centimeters (1.5 inches) per year! This gradual drift is caused by tidal interactions between the Earth and the moon, and over millions\n",
      "---\n",
      "\n",
      "Run 3:\n",
      "---\n",
      "Here’s a surprising fact: The moon is moving away from Earth at a rate of about **3.8 centimeters (1.5 inches) per year**! This slow drift is caused by the tidal interactions between the Earth and the moon. Over\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ── Setup Client ──────────────────────────────────────────────────────────\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version    = \"2024-10-21\"\n",
    ")\n",
    "\n",
    "# ── Deterministic Output with Seed ────────────────────────────────────────\n",
    "for i in range(3):\n",
    "    print(f\"Run {i + 1}:\\n---\")\n",
    "    response = client.chat.completions.create(\n",
    "        model       = \"gpt-4.1\",  # Or your deployment name\n",
    "        seed        = 123,                 # Key to reproducibility\n",
    "        temperature = 0.7,\n",
    "        max_tokens  = 50,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Give me a surprising fact about the moon.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03929b57",
   "metadata": {},
   "source": [
    "#### 1.6 COMPLETIONS API + PREDICTED OUTPUTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d8b4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAST:\n",
      " 1. Purpose. The parties agree to exchange confidential information.\n",
      "2. Term. This agreement is valid for 3 years from the Effective Date.\n",
      "3. Governing Law. This agreement is governed by the laws of England & Wales.\n",
      "...\n",
      "30. Signatures. The parties have executed this Agreement.\n",
      "accepted / rejected prediction tokens: CompletionTokensDetails(accepted_prediction_tokens=2, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=17) \n",
      "\n",
      "SLOW:\n",
      " 1. Purpose. The parties agree to exchange confidential information.\n",
      "2. Term. This agreement is valid for 3 years from the Effective Date.\n",
      "3. Governing Law. This agreement is governed by the laws of England & Wales.\n",
      "...\n",
      "30. Signatures. The parties have executed this Agreement.\n",
      "total fresh tokens generated: 61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ── 1. client ─────────────────────────────────────────────────────────────\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version    = \"2025-01-01-preview\",\n",
    ")\n",
    "DEPLOYMENT = \"gpt-4.1\"   # ← replace with your deployed model name\n",
    "\n",
    "# ── 2. the “big” document ────────────────────────────────────────────────\n",
    "contract = \"\"\"\n",
    "1. Purpose. The parties agree to exchange confidential information.\n",
    "2. Term. This agreement is valid for 1 year from the Effective Date.\n",
    "3. Governing Law. This agreement is governed by the laws of England & Wales.\n",
    "...\n",
    "30. Signatures. The parties have executed this Agreement.\n",
    "\"\"\"\n",
    "\n",
    "# ── 3. we only want to change clause 2 ────────────────────────────────────\n",
    "instruction = \"\"\"\n",
    "Change the clause that says “valid for 1 year” so that it reads “valid for 3 years”.\n",
    "Return the full contract.  No other edits.  No markdown.\n",
    "\"\"\"\n",
    "\n",
    "# ── 4. fast call WITH predicted outputs ───────────────────────────────────\n",
    "fast = client.chat.completions.create(\n",
    "    model    = DEPLOYMENT,\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": contract},\n",
    "    ],\n",
    "    prediction = {                 # tell the model “this text is mostly right already”\n",
    "        \"type\": \"content\",\n",
    "        \"content\": contract\n",
    "    }\n",
    ")\n",
    "print(\"FAST:\\n\", fast.choices[0].message.content)\n",
    "print(\"accepted / rejected prediction tokens:\",\n",
    "      fast.usage.completion_tokens_details, \"\\n\")\n",
    "\n",
    "# ── 5. slower call WITHOUT predicted outputs (for comparison) ─────────────\n",
    "slow = client.chat.completions.create(\n",
    "    model    = DEPLOYMENT,\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": contract},\n",
    "    ]\n",
    ")\n",
    "print(\"SLOW:\\n\", slow.choices[0].message.content)\n",
    "print(\"total fresh tokens generated:\", slow.usage.completion_tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfbbae1",
   "metadata": {},
   "source": [
    "#### 1.7 COMPLETIONS API + PROMPT CACHING \n",
    "\n",
    "When the first 1024 tokens of two chat-completion requests are identical, the service saves the heavy intermediate computations for those tokens in a per-subscription, short-lived cache (5–60 min). On subsequent requests with the same leading tokens, the model skips recomputation, reusing the cached results. Those tokens are billed at a steep discount (free on Provisioned Throughput) and stream back much faster, while only the new or different tokens are processed normally. One-character changes in that initial block invalidate the cache; everything after the first 1 024 tokens qualifies for additional hits in 128-token blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c774cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 1,041 ms\n",
      "Prompt tokens: 2011  (cached: 0)\n",
      "Completion: 'This technical design document outlines the architecture, safety considerations, and verification strategy for a new flight computer system, focusing on fault tolerance and power efficiency.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "prompt_cache_demo.py – minimal example that triggers Azure OpenAI prompt-caching\n",
    "∙ Works on GPT-4.1, GPT-4o, o1/​o3-mini models (API ≥ 2024-10-01-preview)\n",
    "\"\"\"\n",
    "\n",
    "import os, time\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# ── 1. client ────────────────────────────────────────────────────────────\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version    = \"2025-01-01-preview\",\n",
    ")\n",
    "DEPLOYMENT = \"gpt-4.1\"            # ← your deployment name\n",
    "\n",
    "# ── 2. build a prompt >1024 tokens, identical at the start ───────────────\n",
    "base = (\n",
    "    \"In this technical design document we describe the architecture, safety \"\n",
    "    \"considerations, and verification strategy for our next-generation flight \"\n",
    "    \"computer system, with emphasis on fault tolerance and power efficiency.\"\n",
    ")\n",
    "# Repeat until we exceed 1 200 tokens (~60 repeats of ~20 tokens each)\n",
    "long_context = \" \".join([base] * 60)\n",
    "\n",
    "# ── 3. ask a trivial question so the model has to read the whole prompt ──\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise summariser.\"},\n",
    "    {\"role\": \"user\",   \"content\": long_context},\n",
    "    {\"role\": \"user\",   \"content\": \"Give me a 1-sentence summary.\"},\n",
    "]\n",
    "\n",
    "# ── 4. send the request and measure latency ──────────────────────────────\n",
    "t0 = time.perf_counter()\n",
    "rsp = client.chat.completions.create(model=DEPLOYMENT, messages=messages)\n",
    "latency_ms = (time.perf_counter() - t0) * 1_000\n",
    "\n",
    "usage  = rsp.usage\n",
    "cached = usage.prompt_tokens_details.cached_tokens or 0\n",
    "\n",
    "print(f\"Latency: {latency_ms:,.0f} ms\")\n",
    "print(f\"Prompt tokens: {usage.prompt_tokens}  \"\n",
    "      f\"(cached: {cached})\")\n",
    "print(f\"Completion: {rsp.choices[0].message.content!r}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b4c74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import os, time\n",
    "from openai import AzureOpenAI        # pip install \"openai>=1.27.0\"\n",
    "\n",
    "# 1️⃣ Client -----------------------------------------------------------------\n",
    "client = AzureOpenAI(\n",
    "    api_key        = os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version    = \"2025-01-01-preview\",\n",
    ")\n",
    "DEPLOYMENT = \"gpt-4.1\"               # ← your deployment name\n",
    "\n",
    "# 2️⃣ Build a prompt that shares ≥1 024 identical tokens ---------------------\n",
    "base = (\n",
    "    \"In this technical design document we describe the architecture, safety \"\n",
    "    \"considerations, and verification strategy for our next-generation flight \"\n",
    "    \"computer system, with emphasis on fault tolerance and power efficiency.\"\n",
    ")\n",
    "long_context = \" \".join([base] * 60)  # ≈2 400 tokens → plenty for caching\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise summariser.\"},\n",
    "    {\"role\": \"user\",   \"content\": long_context},\n",
    "    {\"role\": \"user\",   \"content\": \"Give me a 1-sentence summary.\"},\n",
    "]\n",
    "\n",
    "# 3️⃣ Helper that calls the model once and prints usage ----------------------\n",
    "def run_once(label: str):\n",
    "    t0 = time.perf_counter()\n",
    "    rsp = client.chat.completions.create(model=DEPLOYMENT, messages=messages)\n",
    "    latency_ms = (time.perf_counter() - t0) * 1_000\n",
    "\n",
    "    # prompt_tokens_details may be None on cache-miss or older models\n",
    "    cached = 0\n",
    "    ptd = getattr(rsp.usage, \"prompt_tokens_details\", None)\n",
    "    if ptd and getattr(ptd, \"cached_tokens\", None):\n",
    "        cached = ptd.cached_tokens\n",
    "\n",
    "    print(\n",
    "        f\"[{label}] latency: {latency_ms:,.0f} ms   \"\n",
    "        f\"prompt tokens: {rsp.usage.prompt_tokens:,} (cached: {cached:,})\"\n",
    "    )\n",
    "    print(\"└─\", rsp.choices[0].message.content, \"\\n\")\n",
    "\n",
    "# 4️⃣ First call populates the cache, second call should be faster ----------\n",
    "run_once(\"cold-start\")   # expect cached_tokens == 0\n",
    "time.sleep(2)            # cache is already warm; pause is just for clarity\n",
    "run_once(\"cache-hit\")    # expect cached_tokens > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb24e8",
   "metadata": {},
   "source": [
    "#### 1.8 FUNCTION CALLING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c21203db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧑  What's the weather in Istanbul?\n",
      "🤖  The current weather in Istanbul is 22°C with clear skies. If you need more detailed or real-time updates, let me know!\n",
      "\n",
      "🧑  Who invented the World Wide Web?\n",
      "🤖  The World Wide Web was invented by Tim Berners-Lee in 1989 while he was working at CERN, the European Organization for Nuclear Research. He developed the first web browser and web server, laying the foundation for the modern internet.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "func_call_demo.py – shortest-possible Azure OpenAI function-calling example\n",
    "∙ Requires API version ≥ 2024-10-01-preview\n",
    "\"\"\"\n",
    "\n",
    "import os, json\n",
    "from openai import AzureOpenAI          # pip install \"openai>=1.27.0\"\n",
    "\n",
    "# ── 1. client ──────────────────────────────────────────────────────────────\n",
    "aoai = AzureOpenAI(\n",
    "    api_key        = os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version    = \"2025-01-01-preview\",\n",
    ")\n",
    "DEPLOYMENT = \"gpt-4.1\"                  # your Azure deployment name\n",
    "\n",
    "# ── 2. declare the callable functions (tools) ──────────────────────────────\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Return weather for a city\",\n",
    "            \"parameters\": {\"type\": \"object\",\n",
    "                           \"properties\": {\"city\": {\"type\":\"string\"}},\n",
    "                           \"required\": [\"city\"]},\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_wikipedia\",\n",
    "            \"description\": \"Brief summary of a factual query\",\n",
    "            \"parameters\": {\"type\": \"object\",\n",
    "                           \"properties\": {\"query\": {\"type\":\"string\"}},\n",
    "                           \"required\": [\"query\"]},\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# ── 3. dummy back-end implementations ──────────────────────────────────────\n",
    "def get_weather(city):        return f\"{city}: 22 °C, clear skies (demo)\"\n",
    "def search_wikipedia(query):  return f\"Wiki says: {query} (demo)\"\n",
    "\n",
    "def dispatch(name, args):     # trivial router\n",
    "    return {\"get_weather\": get_weather,\n",
    "            \"search_wikipedia\": search_wikipedia}[name](**args)\n",
    "\n",
    "# ── 4. chat loop: each prompt triggers its own tool call ───────────────────\n",
    "for prompt in [\"What's the weather in Istanbul?\",\n",
    "               \"Who invented the World Wide Web?\"]:\n",
    "    chain = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    first = aoai.chat.completions.create(\n",
    "        model      = DEPLOYMENT,\n",
    "        messages   = chain,\n",
    "        tools      = tools,\n",
    "        tool_choice= \"auto\",     # let the model decide\n",
    "    ).choices[0]\n",
    "\n",
    "    if first.finish_reason == \"tool_calls\":\n",
    "        tc   = first.message.tool_calls[0]             # only one tool call here\n",
    "        args = json.loads(tc.function.arguments)\n",
    "        out  = dispatch(tc.function.name, args)        # our Python stub result\n",
    "\n",
    "        final = aoai.chat.completions.create(\n",
    "            model    = DEPLOYMENT,\n",
    "            messages = chain + [\n",
    "                first.message,\n",
    "                {\"role\":\"tool\",\n",
    "                 \"tool_call_id\": tc.id,\n",
    "                 \"content\": out}],\n",
    "        ).choices[0].message.content\n",
    "    else:                                              # model answered directly\n",
    "        final = first.message.content\n",
    "\n",
    "    print(f\"\\n🧑  {prompt}\\n🤖  {final}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894b5fe",
   "metadata": {},
   "source": [
    "Below is a function-calling demo that swaps bare JSON schemas for ✧Pydantic✧ models.\n",
    "The trick is BaseModel.model_json_schema() —Pydantic builds the exact parameter schema OpenAI expects, so the same code stays type-safe and self-documenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2edd7391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧑  Weather in Istanbul?\n",
      "🤖  The current weather in Istanbul is 22°C with clear skies. If you need a forecast for the coming days, let me know!\n",
      "\n",
      "🧑  Who invented the World Wide Web?\n",
      "🤖  The World Wide Web was invented by Tim Berners-Lee in 1989. He is a British computer scientist who created the Web while working at CERN (the European Organization for Nuclear Research) to facilitate information sharing among scientists and researchers globally.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "func_call_pydantic_demo.py – Azure OpenAI function-calling with Pydantic\n",
    "∙ Tested with openai≥1.27 + pydantic≥2.6, API version ≥ 2024-10-01-preview\n",
    "\"\"\"\n",
    "\n",
    "import os, json\n",
    "from openai import AzureOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ── 1. client ────────────────────────────────────────────────────────────────\n",
    "aoai = AzureOpenAI(\n",
    "    api_key        = os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version    = \"2025-01-01-preview\",\n",
    ")\n",
    "DEPLOYMENT = \"gpt-4.1\"                         # your deployment name\n",
    "\n",
    "# ── 2. argument models → JSON Schemas (Pydantic does the heavy lifting) ─────\n",
    "class WeatherArgs(BaseModel):\n",
    "    city: str = Field(..., description=\"City name to look up\")\n",
    "\n",
    "class WikiArgs(BaseModel):\n",
    "    query: str = Field(..., description=\"Topic to summarise from Wikipedia\")\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Return simple weather info for a city\",\n",
    "            \"parameters\": WeatherArgs.model_json_schema(),   # ← auto-schema\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_wikipedia\",\n",
    "            \"description\": \"Return a concise Wikipedia summary\",\n",
    "            \"parameters\": WikiArgs.model_json_schema(),\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# ── 3. stub back-end implementations (real code would call APIs) ─────────────\n",
    "def get_weather(city: str)        -> str: return f\"{city}: 22 °C, clear skies (demo)\"\n",
    "def search_wikipedia(query: str)  -> str: return f\"{query}: summary from Wikipedia (demo)\"\n",
    "\n",
    "def dispatch(name:str, payload:dict)->str:\n",
    "    return {\"get_weather\": get_weather,\n",
    "            \"search_wikipedia\": search_wikipedia}[name](**payload)\n",
    "\n",
    "# ── 4. chat loop – two prompts, two different tool calls, one tidy flow ─────\n",
    "for prompt in [\"Weather in Istanbul?\", \"Who invented the World Wide Web?\"]:\n",
    "    chain = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    first = aoai.chat.completions.create(\n",
    "        model       = DEPLOYMENT,\n",
    "        messages    = chain,\n",
    "        tools       = tools,\n",
    "        tool_choice = \"auto\",\n",
    "    ).choices[0]\n",
    "\n",
    "    if first.finish_reason == \"tool_calls\":\n",
    "        tc   = first.message.tool_calls[0]\n",
    "        args = json.loads(tc.function.arguments)      # conforms to Pydantic schema\n",
    "        data = dispatch(tc.function.name, args)\n",
    "\n",
    "        follow = aoai.chat.completions.create(\n",
    "            model    = DEPLOYMENT,\n",
    "            messages = chain + [\n",
    "                first.message,\n",
    "                {\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": data},\n",
    "            ],\n",
    "        )\n",
    "        reply = follow.choices[0].message.content\n",
    "    else:\n",
    "        reply = first.message.content\n",
    "\n",
    "    print(f\"\\n🧑  {prompt}\\n🤖  {reply}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca8d90",
   "metadata": {},
   "source": [
    "\n",
    "### VISION MODELS \n",
    "\n",
    "### REAL-TIME API \n",
    "\n",
    "### AUDIO MODELS \n",
    "\n",
    "### 2. SEMANTIC KERNEL "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
